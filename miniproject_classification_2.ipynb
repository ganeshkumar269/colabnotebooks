{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "miniproject_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "96bc0d5ce81d48298914fdade5b59ea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b1b7ae7b05384a009908569a282cc356",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_46baacba8ed844e9ba500db0618943e6",
              "IPY_MODEL_51519f811f034782a2e5168da663b277"
            ]
          }
        },
        "b1b7ae7b05384a009908569a282cc356": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "46baacba8ed844e9ba500db0618943e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e5cbb8afbd6245158e79ad354b0dd9c3",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102502400,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102502400,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c529f3b4e80a4afdbcecfb302f1192f7"
          }
        },
        "51519f811f034782a2e5168da663b277": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_95668d6a55904885b47de59168682b62",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:02&lt;00:00, 51.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_39cead0ed98e4e6c8698edc9595c9b2f"
          }
        },
        "e5cbb8afbd6245158e79ad354b0dd9c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c529f3b4e80a4afdbcecfb302f1192f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "95668d6a55904885b47de59168682b62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "39cead0ed98e4e6c8698edc9595c9b2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ganeshkumar269/colabnotebooks/blob/main/miniproject_classification_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d1QvscN4-br",
        "outputId": "bc9c26ab-a176-43ba-f59e-b401e371ec91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRsZXp_FtHQC"
      },
      "source": [
        "base_path = '/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/'\n",
        "gt_path = '/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/ISIC-2017_Training_Part3_GroundTruth.csv'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj1FcyEpz-48"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_TmS77xz-1n"
      },
      "source": [
        "i=1\n",
        "while i > 0:\n",
        "  if i%10000 == 0 :\n",
        "    print(i)\n",
        "  if i > 100000000:\n",
        "    i=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGNpfs3hzq9o",
        "cellView": "form"
      },
      "source": [
        "#@title train_mel.py {form-width : \"50px\"}\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils import data\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "RANDOM_SEED = 6666\n",
        "\n",
        "\n",
        "def main():\n",
        "    np.random.seed(RANDOM_SEED)\n",
        "    torch.manual_seed(RANDOM_SEED)\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "    random.seed(RANDOM_SEED)\n",
        "\n",
        "    def train(model, dataloader, criterion, optimizer):\n",
        "        model.train()\n",
        "        losses = []\n",
        "        acc = 0.0\n",
        "        for index, (images, labels, _) in enumerate(dataloader):\n",
        "            labels = labels.to(device).unsqueeze(1).float()\n",
        "            images = images.to(device)\n",
        "            predictions = model(images)\n",
        "            loss = criterion(predictions, labels)\n",
        "            logps = F.logsigmoid(predictions)\n",
        "            ps_ = torch.exp(logps)\n",
        "            equals = torch.ge(ps_, 0.5).float() == labels\n",
        "            acc += equals.sum().item()\n",
        "            losses.append(loss.item())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        train_loss = sum(losses) / len(losses)\n",
        "        train_acc = acc / len(dataloader.dataset)\n",
        "        print(f'\\ntrain_Accuracy: {train_acc:.5f}, train_Loss: {train_loss:.5f}')\n",
        "        return train_acc, train_loss\n",
        "\n",
        "    def validation(model, dataloader, criterion):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            running_acc = 0.0\n",
        "            val_losses = []\n",
        "            for index, (images, labels, _) in enumerate(dataloader, start=1):\n",
        "                labels = labels.to(device).unsqueeze(1).float()\n",
        "                images = images.to(device)\n",
        "                # hogs = hogs.to(device)\n",
        "                score = []\n",
        "                for i in range(len(images[0])):\n",
        "                    ps = model(images[:,i])\n",
        "                    score.append(ps)\n",
        "                score = sum(score) / len(score)\n",
        "                logps = F.logsigmoid(score)\n",
        "                ps_ = torch.exp(logps)\n",
        "                loss = criterion(score, labels)\n",
        "                val_losses.append(loss.item())\n",
        "                equals = torch.ge(ps_, 0.5).float() == labels\n",
        "                running_acc += equals.sum().item()\n",
        "            val_loss = sum(val_losses) / len(val_losses)\n",
        "            val_acc = running_acc / len(dataloader.dataset)\n",
        "            print(f'\\nval_Accuracy: {val_acc:.5f}, val_Loss: {val_loss:.5f}')\n",
        "        return val_acc, val_loss\n",
        "\n",
        "    def save_checkpoint():\n",
        "        filename = os.path.join(checkpoint_dir, \"mel_arlnet50_b32_best_acc.pkl\")\n",
        "        # torch.save(model.state_dict(), filename)\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss\n",
        "            }, filename)\n",
        "\n",
        "    def adjust_learning_rate():\n",
        "        nonlocal lr\n",
        "        lr = lr / lr_decay\n",
        "        return optim.SGD(model.parameters(), lr, weight_decay=weight_decay, momentum=0.9)\n",
        "\n",
        "    # set the parameters\n",
        "    data_dir = '/content/gdrive/My Drive/ISIC-2017-Org-Train-Data'\n",
        "    # Create the dataloaders\n",
        "    batch_size = 32\n",
        "    # the checkpoint dir\n",
        "    checkpoint_dir = \"/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/checkpoint\"\n",
        "\n",
        "    # the learning rate para\n",
        "    lr = 1e-4\n",
        "    lr_decay = 2\n",
        "    weight_decay = 1e-4\n",
        "\n",
        "    stage = 0\n",
        "    start_epoch = 0\n",
        "    stage_epochs = [30, 30, 30, 10]\n",
        "    total_epochs = sum(stage_epochs)\n",
        "    writer_dir = os.path.join(checkpoint_dir, \"mel_arlnet50\")\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "\n",
        "    if not os.path.exists(writer_dir):\n",
        "        os.makedirs(writer_dir)\n",
        "\n",
        "    writer = SummaryWriter(writer_dir)\n",
        "\n",
        "    train_transforms = transforms.Compose([\n",
        "        # transforms.Resize((224, 224)),\n",
        "        transforms.RandomRotation((-10, 10)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.7079057, 0.59156483, 0.54687315],\n",
        "                             std=[0.09372108, 0.11136277, 0.12577087])\n",
        "    ])\n",
        "\n",
        "    val_transforms = argumentation_val()\n",
        "    # training dataset\n",
        "    train_dataset = ISICDataset(path=data_dir, mode=\"training\", crop=None, transform=train_transforms, task=\"mel\")\n",
        "    val_dataset = ISICDataset(path=data_dir, mode=\"validation\", crop=None, transform=val_transforms, task=\"mel\")\n",
        "\n",
        "    train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "    val_loader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
        "    # get the model\n",
        "    model = arlnet50(pretrained=True)\n",
        "\n",
        "    # the loss function\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    # the optimizer\n",
        "    optimizer = optim.SGD(model.parameters(), lr, weight_decay=weight_decay, momentum=0.9)\n",
        "\n",
        "    # initialize the accuracy\n",
        "    acc = 0.0\n",
        "    for epoch in tqdm(range(start_epoch, total_epochs)):\n",
        "\n",
        "        train_acc, train_loss = train(model, train_loader, criterion, optimizer)\n",
        "        val_acc, val_loss = validation(model, val_loader, criterion)\n",
        "        writer.add_scalar(\"train acc\", train_acc, epoch)\n",
        "        writer.add_scalar(\"train loss\", train_loss, epoch)\n",
        "        writer.add_scalar(\"val accuracy\", val_acc, epoch)\n",
        "        writer.add_scalar(\"val loss\", val_loss, epoch)\n",
        "\n",
        "        if val_acc > acc or val_acc == acc:\n",
        "            acc = val_acc\n",
        "            print(\"save the checkpoint, the accuracy of validation is {}\".format(acc))\n",
        "            save_checkpoint()\n",
        "\n",
        "        if (epoch + 1) % 50 == 0:\n",
        "            torch.save(model.state_dict(), \"/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/checkpoint/mel_arlnet50/mel_arlnet50_b32_epoches_{}.pkl\".format(epoch + 1))\n",
        "\n",
        "        if (epoch + 1) in np.cumsum(stage_epochs)[:-1]:\n",
        "            stage += 1\n",
        "            optimizer = adjust_learning_rate()\n",
        "            print('Step into next stage')\n",
        "\n",
        "        if (epoch + 1) == 50:\n",
        "            torch.save(model.state_dict(), \"/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/checkpoint/mel_arlnet50/mel_arlnet50_b32_epoches_{}.pkl\".format(epoch + 1))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKE0MNAjaVCW",
        "cellView": "form"
      },
      "source": [
        "#@title dataset2017.py { form-width: \"50px\" }\n",
        "from torch.utils import data\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "class ISICDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, path, mode=\"training\", crop=None, transform=None, task=None):\n",
        "        self.path = path\n",
        "        self.mode = mode\n",
        "        self.samples = self.make_dataset(path)\n",
        "        self.crop = crop\n",
        "        self.transform = transform\n",
        "        self.task = task\n",
        "        self.image_list = []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, melanoma, seborrheic_keratosis = self.samples[idx]\n",
        "        img_name = img_path.split(\"/\")[-1]\n",
        "        image = self.pil_loader(img_path)\n",
        "        if self.crop:\n",
        "            image = self.crop(image)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.task==\"mel\":\n",
        "            return image, torch.from_numpy(np.array(int(melanoma))), img_name\n",
        "        elif self.task==\"sk\":\n",
        "            return image, torch.from_numpy(np.array(int(seborrheic_keratosis))), img_name\n",
        "        else:\n",
        "            return image, torch.FloatTensor([torch.from_numpy(np.array(int(melanoma))), torch.from_numpy(np.array(int(seborrheic_keratosis)))]), img_name\n",
        "        \n",
        "\n",
        "    def pil_loader(self, path):\n",
        "        with open(path, 'rb') as f:\n",
        "            img = Image.open(f)\n",
        "            return img.convert('RGB')\n",
        "\n",
        "    def make_dataset(self, dir):\n",
        "        images = []\n",
        "        if self.mode == \"training\":\n",
        "            img_dir = os.path.join(dir, \"ISIC-2017_Training_Data_Patch_2\")\n",
        "            csv_filename = os.path.join(dir, \"ISIC-2017_Training_Part3_GroundTruth_patch_2.csv\")\n",
        "        if self.mode == \"validation\":\n",
        "            img_dir = os.path.join(dir, \"After-Enhancement-Val-2\")\n",
        "            csv_filename = os.path.join(dir, \"validation_gt.csv\")\n",
        "        if self.mode == \"testing\":\n",
        "            img_dir = os.path.join(dir, \"ISIC-2017_Test_v2_Data\")\n",
        "            csv_filename = os.path.join(dir, \"ISIC-2017_Test_v2_Part3_GroundTruth.csv\")\n",
        "        label_list = pd.read_csv(csv_filename)\n",
        "\n",
        "        for index, row in label_list.iterrows():\n",
        "            if self.mode == \"training\":\n",
        "                images.append((os.path.join(img_dir, row[\"image_id\"] + \".png\"), row[\"melanoma\"], row[\"seborrheic_keratosis\"]))\n",
        "            else:\n",
        "                images.append((os.path.join(img_dir, row[\"image_id\"] + \".jpg\"), row[\"melanoma\"], row[\"seborrheic_keratosis\"]))\n",
        "        return images"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_ku4bG5arWv",
        "cellView": "form"
      },
      "source": [
        "#@title crop_transform.py { form-width: \"50px\" }\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob as gb\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import warnings\n",
        "from scipy import ndimage\n",
        "import cv2\n",
        "def rescale_crop(image, scale, num, mode):\n",
        "    image_list = []\n",
        "    h, w = image.size\n",
        "    if mode==\"train\":\n",
        "        trans = transforms.Compose([\n",
        "        transforms.CenterCrop((int(h * scale) + 500 * scale, int(w * scale) + 500 * scale)),\n",
        "        transforms.RandomCrop((int(h * scale), int(w * scale))),\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.RandomRotation((-10,10)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "    ])\n",
        "    elif mode==\"val\":\n",
        "        trans = transforms.Compose([\n",
        "            transforms.CenterCrop((int(h * scale) + 500 * scale, int(w * scale) + 500 * scale)),\n",
        "            transforms.RandomCrop((int(h * scale), int(w * scale))),\n",
        "            transforms.Resize((224, 224)),\n",
        "        ])\n",
        "    for i in range(num):\n",
        "        img = trans(image)\n",
        "        image_list.append(img)\n",
        "    return image_list\n",
        "\n",
        "def crop(image, mode):\n",
        "    image_list = []\n",
        "    if mode==\"train\":\n",
        "        trans = transforms.Compose([\n",
        "\n",
        "        transforms.RandomRotation((-10, 10)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.Resize((224, 224)),  #change the order\n",
        "    ])\n",
        "    elif mode==\"val\":\n",
        "        trans = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "        ])\n",
        "    img = trans(image)\n",
        "    image_list.append(img)\n",
        "    return image_list\n",
        "\n",
        "class argumentation(object):\n",
        "    def __call__(self, image):\n",
        "        image_list1 = rescale_crop(image, 0.2, 15, \"train\")\n",
        "        image_list2 = rescale_crop(image, 0.4, 15, \"train\")\n",
        "        image_list3 = rescale_crop(image, 0.6, 15, \"train\")\n",
        "        image_list4 = rescale_crop(image, 0.8, 15, \"train\")\n",
        "        image_list5 = crop(image, \"train\")\n",
        "        image_list = image_list1 + image_list2 + image_list3 + image_list4 + image_list5\n",
        "        nomalize = transforms.Lambda(lambda crops: torch.stack([transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.7079057, 0.59156483, 0.54687315],\n",
        "                             std=[0.09372108, 0.11136277, 0.12577087])])(crop) for crop in crops]))\n",
        "        random.shuffle(image_list)\n",
        "        image_list = nomalize(image_list)\n",
        "        return image_list\n",
        "\n",
        "class argumentation_val(object):\n",
        "    def __call__(self, image):\n",
        "        image_list1 = rescale_crop(image, 0.2, 2, \"val\")\n",
        "        image_list2 = rescale_crop(image, 0.4, 2, \"val\")\n",
        "        image_list3 = rescale_crop(image, 0.6, 2, \"val\")\n",
        "        image_list4 = rescale_crop(image, 0.8, 2, \"val\")\n",
        "        image_list5 = crop(image, \"val\")\n",
        "        image_list = image_list1 + image_list2 + image_list3 + image_list4 + image_list5\n",
        "        nomalize = transforms.Lambda(lambda crops: torch.stack([transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.7079057, 0.59156483, 0.54687315],\n",
        "                             std=[0.09372108, 0.11136277, 0.12577087])])(crop) for crop in crops]))\n",
        "        image_list = nomalize(image_list)\n",
        "        return image_list"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmvWX1nea1o4",
        "cellView": "form",
        "outputId": "b766a56a-898a-41c0-d082-428985471bc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title generate_patch_images.py { form-width: \"50px\" }\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import glob as gb\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "def get_images(root_path):\n",
        "    files = []\n",
        "    for ext in ['jpg']:\n",
        "        files.extend(gb.glob(os.path.join(root_path, '*.{}'.format(ext))))\n",
        "    return files\n",
        "\n",
        "def rescale_crop(image, scale, num, ori=False):\n",
        "    image_list = []\n",
        "    h, w = image.size\n",
        "    if ori:\n",
        "        trans = transforms.Resize((224,224))\n",
        "    else:\n",
        "        trans = transforms.Compose([\n",
        "        transforms.CenterCrop((int(h * scale) + 500 * scale, int(w * scale) + 500 * scale)),\n",
        "        transforms.RandomCrop((int(h * scale), int(w * scale))),\n",
        "        transforms.Resize((224,224))\n",
        "    ])\n",
        "    for i in range(num):\n",
        "        img = trans(image)\n",
        "        image_list.append(img)\n",
        "    return image_list\n",
        "\n",
        "data_dir = \"/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/ISIC-2017_Training_Data/\"\n",
        "new_data_dir = base_path +'ISIC-2017_Training_Data_Patch_2/'\n",
        "excel_dir = base_path + \"ISIC-2017_Training_Part3_GroundTruth.csv\"\n",
        "new_excel_dir = base_path + \"ISIC-2017_Training_Part3_GroundTruth_patch_2.csv\"\n",
        "images = get_images(data_dir)\n",
        "\n",
        "if not os.path.exists(new_data_dir):\n",
        "    os.makedirs(new_data_dir)\n",
        "\n",
        "\n",
        "ids = []\n",
        "mels = []\n",
        "sks = []\n",
        "for img in tqdm(images):\n",
        "    image = Image.open(img)\n",
        "    labels = pd.read_csv(excel_dir)\n",
        "    mel = int(labels.loc[labels['image_id'] == img[img.rfind('/')+1:-4]]['melanoma'].values.squeeze())\n",
        "    sk = int(labels.loc[labels['image_id'] == img[img.rfind('/')+1:-4]]['seborrheic_keratosis'].values.squeeze())\n",
        "    image_list1 = rescale_crop(image, 0.2, 15)\n",
        "    image_list2 = rescale_crop(image, 0.4, 15)\n",
        "    image_list3 = rescale_crop(image, 0.6, 15)\n",
        "    image_list4 = rescale_crop(image, 0.8, 15)\n",
        "    image_list5 = rescale_crop(image, 1, 1, True)\n",
        "    image_list_all = image_list1 + image_list2 + image_list3 + image_list4 + image_list5\n",
        "\n",
        "    for i in range(len(image_list_all)):\n",
        "        new_name = img[img.rfind('/')+1:-4] + '_' + str(i) + '.png'\n",
        "        new_dir = os.path.join(new_data_dir, new_name)\n",
        "        image_list_all[i].save(new_dir)\n",
        "        labels = pd.read_csv(excel_dir)\n",
        "        ids.append(new_name[:-4])\n",
        "        mels.append(mel)\n",
        "        sks.append(sk)\n",
        "data_frame = pd.DataFrame({\"image_id\": ids, \"melanoma\": mels, \"seborrheic_keratosis\": sks})\n",
        "data_frame.to_csv(new_excel_dir, index=False, sep=\",\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [2:13:06<00:00,  3.99s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlMUvkb2a91b",
        "cellView": "form",
        "outputId": "2668e364-44a8-4c0a-fd89-8392439b3549",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "#@title predict_2017.py { form-width: \"50px\" }\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "# from dataset2017 import ISICDataset\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "# from crop_transform import *\n",
        "# from models.ARL import arlnet50\n",
        "RANDOM_SEED = 6666\n",
        "\n",
        "\n",
        "def main():\n",
        "    np.random.seed(RANDOM_SEED)\n",
        "    torch.manual_seed(RANDOM_SEED)\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "    random.seed(RANDOM_SEED)\n",
        "\n",
        "    # set the parameters\n",
        "    checkpoint_dir = base_path + \"checkpoint/mel_arlnet50_b32_best_acc_third.pkl\"\n",
        "    result_dir = base_path + \"result\"\n",
        "    data_dir = base_path\n",
        "    # Create the dataloaders\n",
        "    batch_size = 1\n",
        "\n",
        "    y = []\n",
        "    y_score = []\n",
        "\n",
        "    if not os.path.exists(result_dir):\n",
        "        os.makedirs(result_dir)\n",
        "\n",
        "    def imshow(y_pre, y_score):\n",
        "        fpr, tpr, thresholds = metrics.roc_curve(y_pre, y_score)\n",
        "        auc = metrics.auc(fpr, tpr)\n",
        "        print(auc)\n",
        "\n",
        "        plt.plot(fpr, tpr, c='r', lw=2, alpha=0.7, label=u'AUC=%.3f' % auc)\n",
        "        plt.plot((0, 1), (0, 1), c='#808080', lw=1, ls='--', alpha=0.7)\n",
        "        plt.xlim((-0.01, 1.02))\n",
        "        plt.ylim((-0.01, 1.02))\n",
        "        plt.xticks(np.arange(0, 1.1, 0.1))\n",
        "        plt.yticks(np.arange(0, 1.1, 0.1))\n",
        "        plt.xlabel('False Positive Rate', fontsize=13)\n",
        "        plt.ylabel('True Positive Rate', fontsize=13)\n",
        "        plt.grid(b=True, ls=':')\n",
        "        plt.legend(loc='lower right', fancybox=True, framealpha=0.8, fontsize=12)\n",
        "        plt.title(u'ROC and AUC', fontsize=17)\n",
        "        plt.savefig(\"/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/ISIC-2017_Training_Data/2017_mel_arlnet50_e100_b32_third.png\")\n",
        "\n",
        "\n",
        "    def load_checkpoint(checkpoint_path):\n",
        "\n",
        "        # Here put the pretrained model that you used (in my case it's densenet161).\n",
        "\n",
        "        # model = resnet50()\n",
        "        # # model = danet()\n",
        "        # # model = resnet50_cbam(pretrained=False)\n",
        "        # # model = se_resnet50()\n",
        "        # # model = proposed()\n",
        "        # # model = models.resnext50_32x4d(pretrained=False)\n",
        "        # try:\n",
        "        #     n_ftrs = model.classifier.in_features\n",
        "        #     model.classifier = classifier(n_ftrs)\n",
        "        # except AttributeError:\n",
        "        #     n_ftrs = model.fc.in_features\n",
        "        #     model.fc = classifier(n_ftrs)\n",
        "        # model = model.to(device)\n",
        "        '''\n",
        "        fn1 = FeatureNet_1()\n",
        "        fn2 = FeatureNet_2()\n",
        "        cfn = ClassifierNet(fn1, fn2) \n",
        "        model = cfn\n",
        "        '''\n",
        "        model = arlnet50(pretrained=True)\n",
        "        # checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "        checkpoint_t = torch.load(checkpoint_path, map_location='cuda')\n",
        "        '''\n",
        "        from collections import OrderedDict\n",
        "        new_state_dict = OrderedDict()\n",
        "        for k, v in checkpoint.items():\n",
        "            name = k[7:]  # remove module.\n",
        "            new_state_dict[name] = v\n",
        "        '''\n",
        "        model.load_state_dict(checkpoint_t['model_state_dict'])  # your checkpoint's key may differ (e.g.'state_dict')\n",
        "        model.eval()\n",
        "        return model\n",
        "\n",
        "    def predict(model, dataloader):\n",
        "        mel_tn = 0\n",
        "        mel_fp = 0\n",
        "        mel_tp = 0\n",
        "        mel_fn = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for ii, (images, labels, _) in tqdm(enumerate(dataloader, start=1)):\n",
        "                images = images.to(device)\n",
        "                scores = []\n",
        "\n",
        "                for i in range(len(images[0])):\n",
        "                    pred = model(images[:, i])\n",
        "                    scores.append(pred)\n",
        "                scores = sum(scores) / len(scores)\n",
        "                logps = F.logsigmoid(scores)\n",
        "                score = torch.exp(logps)\n",
        "                pre = torch.ge(score, 0.5).float()\n",
        "                if int(pre) == 0 and int(labels) == 0:\n",
        "                    mel_tn += 1\n",
        "                elif int(pre) == 1 and int(labels) == 0:\n",
        "                    mel_fp += 1\n",
        "                elif int(pre) == 1 and int(labels) == 1:\n",
        "                    mel_tp += 1\n",
        "                elif int(pre) == 0 and int(labels) == 1:\n",
        "                    mel_fn += 1\n",
        "                score = score.cpu().numpy().tolist()[0]\n",
        "                label = labels.cpu().numpy().tolist()[0]\n",
        "                y.append(label)\n",
        "                y_score.append(score)\n",
        "        return mel_tp, mel_tn, mel_fp, mel_fn\n",
        "\n",
        "    val_transforms = argumentation_val()\n",
        "    # Validation dataset\n",
        "    val_dataset = ISICDataset(path=data_dir, mode=\"testing\", crop=None, transform=val_transforms, task=\"mel\")\n",
        "    val_loader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    model = load_checkpoint(checkpoint_path=checkpoint_dir)\n",
        "\n",
        "    model.cuda()\n",
        "    mel_tp, mel_tn, mel_fp, mel_fn = predict(model, val_loader)\n",
        "\n",
        "    mel_acc = (mel_tp + mel_tn) / (mel_tn + mel_fp + mel_tp + mel_fn)\n",
        "    mel_sen = mel_tp / (mel_tp + mel_fn)\n",
        "    mel_spe = mel_tn / (mel_tn + mel_fp)\n",
        "\n",
        "    y_score = np.array(y_score)\n",
        "    mel_auc = metrics.roc_auc_score(y, y_score)\n",
        "\n",
        "    imshow(y, y_score)\n",
        "\n",
        "    print('mel_Accuracy:', mel_acc)\n",
        "    print('mel_Sensitive:', mel_sen)\n",
        "    print('mel_Specificity:', mel_spe)\n",
        "    print('mel_AUC:', mel_auc)\n",
        "    with open('/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/result/result_third.txt', 'a') as f:\n",
        "        f.write('\\n2017_mel_arlnet50_e100_b32: ' + json.dumps(\n",
        "            {'mel_Accuracy': mel_acc, 'mel_Sensitive': mel_sen, 'mel_Specificity': mel_spe, 'mel_AUC': mel_auc}))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "600it [16:25,  1.64s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8522588522588522\n",
            "mel_Accuracy: 0.8516666666666667\n",
            "mel_Sensitive: 0.4358974358974359\n",
            "mel_Specificity: 0.9523809523809523\n",
            "mel_AUC: 0.8522588522588522\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDG3TrA3-TYX",
        "cellView": "form",
        "outputId": "53f89692-4f6a-450a-a82b-824de717beff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "96bc0d5ce81d48298914fdade5b59ea9",
            "b1b7ae7b05384a009908569a282cc356",
            "46baacba8ed844e9ba500db0618943e6",
            "51519f811f034782a2e5168da663b277",
            "e5cbb8afbd6245158e79ad354b0dd9c3",
            "c529f3b4e80a4afdbcecfb302f1192f7",
            "95668d6a55904885b47de59168682b62",
            "39cead0ed98e4e6c8698edc9595c9b2f"
          ]
        }
      },
      "source": [
        "#@title ARL.py {form-width:\"50px\"}\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "import torch.nn.init\n",
        "\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, alpha =0.001):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        attention = nn.Softmax2d(out)\n",
        "\n",
        "        out = out + residual + self.alpha * attention * residual\n",
        "\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, alpha = 0.001):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        attention = nn.Softmax2d()(out)\n",
        "\n",
        "        out = out + residual + self.alpha * attention *residual\n",
        "\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ARLNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1):\n",
        "        self.inplanes = 64\n",
        "        super(ARLNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])    #3\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2) #4\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)  #6\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)  #3\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc_ = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def arlnet18(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ARLNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "    if pretrained:\n",
        "        model_pretrained = model_zoo.load_url(model_urls['resnet18'])\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_dict = {k: v for k, v in model_pretrained.items() if k in model_dict}\n",
        "        model_dict.update(pretrained_dict)\n",
        "        model.load_state_dict(model_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def arlnet34(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ARLNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model_pretrained = model_zoo.load_url(model_urls['resnet34'])\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_dict = {k: v for k, v in model_pretrained.items() if k in model_dict}\n",
        "        model_dict.update(pretrained_dict)\n",
        "        model.load_state_dict(model_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def arlnet50(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-50 model.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ARLNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model_pretrained = model_zoo.load_url(model_urls['resnet50'])\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_dict = {k: v for k, v in model_pretrained.items() if k in model_dict}\n",
        "        model_dict.update(pretrained_dict)\n",
        "        model.load_state_dict(model_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def arlnet101(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ARLNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model_pretrained = model_zoo.load_url(model_urls['resnet101'])\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_dict = {k: v for k, v in model_pretrained.items() if k in model_dict}\n",
        "        model_dict.update(pretrained_dict)\n",
        "        model.load_state_dict(model_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def arlnet152(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-152 model.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ARLNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model_pretrained = model_zoo.load_url(model_urls['resnet152'])\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_dict = {k: v for k, v in model_pretrained.items() if k in model_dict}\n",
        "        model_dict.update(pretrained_dict)\n",
        "        model.load_state_dict(model_dict)\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = arlnet50(pretrained=True)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96bc0d5ce81d48298914fdade5b59ea9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=102502400.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QU5qM2H6-cdl",
        "cellView": "form"
      },
      "source": [
        "#@title resnet.py {form-width:\"50px\"}\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    __constants__ = ['downsample']\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    __constants__ = ['downsample']\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    # Allow for accessing forward method in a inherited class\n",
        "    forward = _forward\n",
        "\n",
        "\n",
        "def _resnet(arch, block, layers, progress, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet18(progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-18 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def resnet34(progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-34 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def resnet50(progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-50 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def resnet101(progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-101 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def resnet152(progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-152 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def resnext50_32x4d(progress=True, **kwargs):\n",
        "    r\"\"\"ResNeXt-50 32x4d model from\n",
        "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs['groups'] = 32\n",
        "    kwargs['width_per_group'] = 4\n",
        "    return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3], progress, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def resnext101_32x8d(progress=True, **kwargs):\n",
        "    r\"\"\"ResNeXt-101 32x8d model from\n",
        "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs['groups'] = 32\n",
        "    kwargs['width_per_group'] = 8\n",
        "    return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3], progress, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def wide_resnet50_2(progress=True, **kwargs):\n",
        "    r\"\"\"Wide ResNet-50-2 model from\n",
        "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n",
        "\n",
        "    The model is the same as ResNet except for the bottleneck number of channels\n",
        "    which is twice larger in every block. The number of channels in outer 1x1\n",
        "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
        "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs['width_per_group'] = 64 * 2\n",
        "    return _resnet('wide_resnet50_2', Bottleneck, [3, 4, 6, 3], progress, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def wide_resnet101_2(progress=True, **kwargs):\n",
        "    r\"\"\"Wide ResNet-101-2 model from\n",
        "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n",
        "\n",
        "    The model is the same as ResNet except for the bottleneck number of channels\n",
        "    which is twice larger in every block. The number of channels in outer 1x1\n",
        "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
        "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs['width_per_group'] = 64 * 2\n",
        "    return _resnet('wide_resnet101_2', Bottleneck, [3, 4, 23, 3], progress, **kwargs)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JliBbR5p_uaT",
        "cellView": "form",
        "outputId": "7e6fd5e4-4a6f-47ac-f4c3-c98c393b4a46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title training-validation split { form-width: \"50px\" }\n",
        "#374-melanoma\n",
        "#254-saborrheic\n",
        "#1372-nevus\n",
        "nev = 1372*0.3\n",
        "sab = 254*0.3\n",
        "mel = 374*0.3\n",
        "import shutil,os\n",
        "import pandas as pd\n",
        "files = os.listdir(base_path + 'After-Segmentation-2')\n",
        "\n",
        "gt = pd.read_csv(gt_path)\n",
        "\n",
        "for i in range(2000):\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzezRX6cvpHI"
      },
      "source": [
        "!unzip -d /content/gdrive/My\\ Drive/ISIC-2017-Org-Train-Data/ /content/gdrive/My\\ Drive/ISIC-2017-Org-Train-Data/ISIC-2017_Test_v2_Part1_GroundTruth.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_P7EfMBhMwE",
        "outputId": "645b9bd3-1000-43d7-d8dd-b7b8db7b8595",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from PIL import Image\n",
        "im = Image.open(base_path+'After-Enhancement-2/ISIC_0000000.jpg')\n",
        "im.size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(256, 256)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4A6QXzFEA5S",
        "outputId": "5fdeaa21-bbd6-4025-95f6-a743238d803d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls /content/gdrive/My\\ Drive/ISIC-2017-Org-Train-Data/ISIC-2017_Training_Data_Patch_2/ | wc -l"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot open directory '/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/ISIC-2017_Training_Data_Patch_2/': Input/output error\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQx7Naqkajx4"
      },
      "source": [
        "!unzip -d /content/gdrive/My\\ Drive/ISIC-2017-Org-Train-Data/ /content/gdrive/My\\ Drive/ISIC-2017-Org-Test-Data/ISIC-2017_Test_v2_Data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7JicIQcusJN"
      },
      "source": [
        "!mkdir /content/gdrive/My\\ Drive/ISIC-2017-Org-Train-Data/ISIC-2017_Training_Data_Patch/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3FtInShuuxd",
        "outputId": "a8bf9fbd-decb-4e66-c271-f087e6312ae6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!rm -rf /content/gdrive/My\\ Drive/ISIC-2017-Org-Train-Data/ISIC-2017_Training_Data_Patchasdasd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exXxwUUr5c2w",
        "outputId": "cc97da44-f3d8-4559-adce-d47c94bda353",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!du -sh /content/gdrive/My\\ Drive/ISIC-2017/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16G\t/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZJyVVnaMTlq",
        "outputId": "f3dc69fe-6c26-4709-c750-6d237b5fbb2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!du -sh /content/gdrive/My\\ Drive/train/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6.5G\t/content/gdrive/My Drive/train/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnjiP9quMZzM",
        "outputId": "f4866033-9c8e-4a24-d3c3-6221c7074753",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!du -sh /content/gdrive/My\\ Drive/test/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.6G\t/content/gdrive/My Drive/test/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b_HUxFAMpzj",
        "outputId": "2ddd2e34-5cb9-4703-b78b-6ea461efdfaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!du -sh /content/gdrive/My\\ Drive/validate/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.7G\t/content/gdrive/My Drive/validate/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0_9HfMCM_v_",
        "outputId": "4af22e76-009e-4b58-e72e-5b5567c71ca1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!du -sh /content/gdrive/My\\ Drive/ISIC-2017-Org-Train-Data/ISIC-2017_Training_Data_Patch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.7G\t/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/ISIC-2017_Training_Data_Patch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBnIc57CNf1D",
        "outputId": "f9050794-d392-4c91-9e0d-06b16df8e741",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from PIL import Image\n",
        "Image.open(base_path+'After-Enhancement-2/ISIC_0000000.jpg').size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(256, 256)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29ofL12XtXYF",
        "outputId": "e5cc14ff-ae63-4470-f362-b50ca6a33ab3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "torch.cuda.current_device()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mwKVQ1MtYQ_",
        "outputId": "47df120b-fd1d-40c7-ab0f-f62eaebe4c40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ls /content/gdrive/My\\ Drive/ISIC-2017-Org-Train-Data/ISIC-2017_Test_v2_Data | wc -l"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1201\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHBXIRwBtbEa",
        "cellView": "form"
      },
      "source": [
        "#@title Train_Mel-2.py  { form-width: \"50px\" }\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils import data\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "RANDOM_SEED = 6666\n",
        "\n",
        "\n",
        "def main():\n",
        "    np.random.seed(RANDOM_SEED)\n",
        "    torch.manual_seed(RANDOM_SEED)\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "    random.seed(RANDOM_SEED)\n",
        "\n",
        "    def train(model, dataloader, criterion, optimizer):\n",
        "        model.train()\n",
        "        losses = []\n",
        "        acc = 0.0\n",
        "        for index, (images, labels, _) in enumerate(dataloader):\n",
        "            labels = labels.to(device).unsqueeze(1).float()\n",
        "            images = images.to(device)\n",
        "            predictions = model(images)\n",
        "            loss = criterion(predictions, labels)\n",
        "            logps = F.logsigmoid(predictions)\n",
        "            ps_ = torch.exp(logps)\n",
        "            equals = torch.ge(ps_, 0.5).float() == labels\n",
        "            acc += equals.sum().item()\n",
        "            losses.append(loss.item())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        train_loss = sum(losses) / len(losses)\n",
        "        train_acc = acc / len(dataloader.dataset)\n",
        "        print(f'\\ntrain_Accuracy: {train_acc:.5f}, train_Loss: {train_loss:.5f}')\n",
        "        return train_acc, train_loss\n",
        "\n",
        "    def validation(model, dataloader, criterion):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            running_acc = 0.0\n",
        "            val_losses = []\n",
        "            for index, (images, labels, _) in enumerate(dataloader, start=1):\n",
        "                labels = labels.to(device).unsqueeze(1).float()\n",
        "                images = images.to(device)\n",
        "                # hogs = hogs.to(device)\n",
        "                score = []\n",
        "                for i in range(len(images[0])):\n",
        "                    ps = model(images[:,i])\n",
        "                    score.append(ps)\n",
        "                score = sum(score) / len(score)\n",
        "                logps = F.logsigmoid(score)\n",
        "                ps_ = torch.exp(logps)\n",
        "                loss = criterion(score, labels)\n",
        "                val_losses.append(loss.item())\n",
        "                equals = torch.ge(ps_, 0.5).float() == labels\n",
        "                running_acc += equals.sum().item()\n",
        "            val_loss = sum(val_losses) / len(val_losses)\n",
        "            val_acc = running_acc / len(dataloader.dataset)\n",
        "            print(f'\\nval_Accuracy: {val_acc:.5f}, val_Loss: {val_loss:.5f}')\n",
        "        return val_acc, val_loss\n",
        "\n",
        "    def save_checkpoint(epoch,acc,lr):\n",
        "        filename = os.path.join(checkpoint_dir, \"mel_arlnet50_b32_best_acc.pkl\")\n",
        "        # torch.save(model.state_dict(), filename)\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'acc': acc,\n",
        "            'lr':lr\n",
        "            }, filename)\n",
        "\n",
        "    def adjust_learning_rate():\n",
        "        nonlocal lr\n",
        "        lr = lr / lr_decay\n",
        "        return optim.SGD(model.parameters(), lr, weight_decay=weight_decay, momentum=0.9)\n",
        "\n",
        "    # set the parameters\n",
        "    data_dir = '/content/gdrive/My Drive/ISIC-2017-Org-Train-Data'\n",
        "    # Create the dataloaders\n",
        "    batch_size = 32\n",
        "    # the checkpoint dir\n",
        "    checkpoint_dir = \"/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/checkpoint\"\n",
        "\n",
        "    # the learning rate para\n",
        "    lr = 1e-4\n",
        "    lr_decay = 2\n",
        "    weight_decay = 1e-4\n",
        "\n",
        "    stage = 0\n",
        "    start_epoch = 0\n",
        "    stage_epochs = [30, 30]\n",
        "    total_epochs = sum(stage_epochs)\n",
        "    writer_dir = os.path.join(checkpoint_dir, \"mel_arlnet50\")\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "\n",
        "    if not os.path.exists(writer_dir):\n",
        "        os.makedirs(writer_dir)\n",
        "\n",
        "    writer = SummaryWriter(writer_dir)\n",
        "\n",
        "    train_transforms = transforms.Compose([\n",
        "        # transforms.Resize((224, 224)),\n",
        "        transforms.RandomRotation((-10, 10)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.7079057, 0.59156483, 0.54687315],\n",
        "                             std=[0.09372108, 0.11136277, 0.12577087])\n",
        "    ])\n",
        "\n",
        "    val_transforms = argumentation_val()\n",
        "    # training dataset\n",
        "    train_dataset = ISICDataset(path=data_dir, mode=\"training\", crop=None, transform=train_transforms, task=\"mel\")\n",
        "    val_dataset = ISICDataset(path=data_dir, mode=\"validation\", crop=None, transform=val_transforms, task=\"mel\")\n",
        "    \n",
        "    # train_sampler = MySampler(train_dataset,last_index)\n",
        "    train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "    val_loader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
        "    # get the model\n",
        "    model = arlnet50(pretrained=True)\n",
        "\n",
        "    # the loss function\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    # the optimizer\n",
        "    optimizer = optim.SGD(model.parameters(), lr, weight_decay=weight_decay, momentum=0.9)\n",
        "\n",
        "    # initialize the accuracy\n",
        "    acc = 0.0\n",
        "    flag = True\n",
        "    if flag:\n",
        "          filename = os.path.join(checkpoint_dir, \"mel_arlnet50_b32_best_acc_third.pkl\")\n",
        "          checkpoint_t = torch.load(filename)\n",
        "          model.load_state_dict(checkpoint_t['model_state_dict'])\n",
        "          optimizer.load_state_dict(checkpoint_t['optimizer_state_dict'])\n",
        "          start_epoch = checkpoint_t['epoch']\n",
        "          lr = checkpoint_t['lr']\n",
        "          acc = checkpoint_t['acc']\n",
        "    for epoch in tqdm(range(start_epoch, total_epochs)):\n",
        "        train_acc, train_loss = train(model, train_loader, criterion, optimizer)\n",
        "        val_acc, val_loss = validation(model, val_loader, criterion)\n",
        "        writer.add_scalar(\"train acc\", train_acc, epoch)\n",
        "        writer.add_scalar(\"train loss\", train_loss, epoch)\n",
        "        writer.add_scalar(\"val accuracy\", val_acc, epoch)\n",
        "        writer.add_scalar(\"val loss\", val_loss, epoch)\n",
        "\n",
        "        if val_acc > acc or val_acc == acc:\n",
        "            acc = val_acc\n",
        "            print(\"save the checkpoint, the accuracy of validation is {}\".format(acc))\n",
        "            save_checkpoint(epoch,acc,lr)\n",
        "\n",
        "        if (epoch + 1) % 50 == 0:\n",
        "            torch.save(model.state_dict(), \"/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/checkpoint/mel_arlnet50/mel_arlnet50_b32_epoches_{}.pkl\".format(epoch + 1))\n",
        "\n",
        "        if (epoch + 1) in np.cumsum(stage_epochs)[:-1]:\n",
        "            stage += 1\n",
        "            optimizer = adjust_learning_rate()\n",
        "            print('Step into next stage')\n",
        "\n",
        "        if (epoch + 1) == 50:\n",
        "            torch.save(model.state_dict(), \"/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/checkpoint/mel_arlnet50/mel_arlnet50_b32_epoches_{}.pkl\".format(epoch + 1))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBc6F8bGBXuR",
        "cellView": "form"
      },
      "source": [
        "#@title MySampler { form-width:\"50px\" }\n",
        "import random\n",
        "from torch.utils.data.dataloader import Sampler\n",
        "\n",
        "random.seed(6666)\n",
        "\n",
        "class MySampler(Sampler):\n",
        "  def __init__(self,data,i=0):\n",
        "    random.shuffle(data)\n",
        "    self.seq = list(range(len(data)))[i*32]:\n",
        "  def __iter__(self):\n",
        "    return iter(self.seq)\n",
        "  def __len__(self):\n",
        "    return len(self.seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu_DvivOIrks"
      },
      "source": [
        "!cp /content/gdrive/My\\ Drive/ISIC-2017-Org-Train-Data/checkpoint/mel_arlnet50_b32_best_acc.pkl /content/gdrive/My\\ Drive/ISIC-2017-Org-Train-Data/checkpoint/mel_arlnet50_b32_best_acc_third.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLs9Y15N2woy",
        "outputId": "00f83bbd-752b-486b-dfe3-2cc4d80307ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-768276c6-720c-7999-02f0-0fff123cdf2e)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uDhV_eQ7mik"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}