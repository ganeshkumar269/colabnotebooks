{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "miniproject_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOvNwBWEoE9BQ5+j449o4QA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ganeshkumar269/colabnotebooks/blob/main/miniproject_classification_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d1QvscN4-br",
        "outputId": "8d8aa7b5-ed13-49a9-9fce-124c5c5d5529",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRsZXp_FtHQC"
      },
      "source": [
        "base_path = '/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/'\n",
        "gt_path = '/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/ISIC-2017_Training_Part3_GroundTruth.csv'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj1FcyEpz-48"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_TmS77xz-1n"
      },
      "source": [
        "i=1\n",
        "while i > 0:\n",
        "  if i%10000 == 0 :\n",
        "    print(i)\n",
        "  if i > 100000000:\n",
        "    i=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGNpfs3hzq9o",
        "cellView": "form"
      },
      "source": [
        "#@title train_mel.py {form-width : \"50px\"}\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils import data\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "RANDOM_SEED = 6666\n",
        "\n",
        "\n",
        "def main():\n",
        "    np.random.seed(RANDOM_SEED)\n",
        "    torch.manual_seed(RANDOM_SEED)\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "    random.seed(RANDOM_SEED)\n",
        "\n",
        "    def train(model, dataloader, criterion, optimizer):\n",
        "        model.train()\n",
        "        losses = []\n",
        "        acc = 0.0\n",
        "        for index, (images, labels, _) in enumerate(dataloader):\n",
        "            labels = labels.to(device).unsqueeze(1).float()\n",
        "            images = images.to(device)\n",
        "            predictions = model(images)\n",
        "            loss = criterion(predictions, labels)\n",
        "            logps = F.logsigmoid(predictions)\n",
        "            ps_ = torch.exp(logps)\n",
        "            equals = torch.ge(ps_, 0.5).float() == labels\n",
        "            acc += equals.sum().item()\n",
        "            losses.append(loss.item())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        train_loss = sum(losses) / len(losses)\n",
        "        train_acc = acc / len(dataloader.dataset)\n",
        "        print(f'\\ntrain_Accuracy: {train_acc:.5f}, train_Loss: {train_loss:.5f}')\n",
        "        return train_acc, train_loss\n",
        "\n",
        "    def validation(model, dataloader, criterion):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            running_acc = 0.0\n",
        "            val_losses = []\n",
        "            for index, (images, labels, _) in enumerate(dataloader, start=1):\n",
        "                labels = labels.to(device).unsqueeze(1).float()\n",
        "                images = images.to(device)\n",
        "                # hogs = hogs.to(device)\n",
        "                score = []\n",
        "                for i in range(len(images[0])):\n",
        "                    ps = model(images[:,i])\n",
        "                    score.append(ps)\n",
        "                score = sum(score) / len(score)\n",
        "                logps = F.logsigmoid(score)\n",
        "                ps_ = torch.exp(logps)\n",
        "                loss = criterion(score, labels)\n",
        "                val_losses.append(loss.item())\n",
        "                equals = torch.ge(ps_, 0.5).float() == labels\n",
        "                running_acc += equals.sum().item()\n",
        "            val_loss = sum(val_losses) / len(val_losses)\n",
        "            val_acc = running_acc / len(dataloader.dataset)\n",
        "            print(f'\\nval_Accuracy: {val_acc:.5f}, val_Loss: {val_loss:.5f}')\n",
        "        return val_acc, val_loss\n",
        "\n",
        "    def save_checkpoint():\n",
        "        filename = os.path.join(checkpoint_dir, \"mel_arlnet50_b32_best_acc.pkl\")\n",
        "        # torch.save(model.state_dict(), filename)\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss\n",
        "            }, filename)\n",
        "\n",
        "    def adjust_learning_rate():\n",
        "        nonlocal lr\n",
        "        lr = lr / lr_decay\n",
        "        return optim.SGD(model.parameters(), lr, weight_decay=weight_decay, momentum=0.9)\n",
        "\n",
        "    # set the parameters\n",
        "    data_dir = '/content/gdrive/My Drive/ISIC-2017-Org-Train-Data'\n",
        "    # Create the dataloaders\n",
        "    batch_size = 32\n",
        "    # the checkpoint dir\n",
        "    checkpoint_dir = \"/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/checkpoint\"\n",
        "\n",
        "    # the learning rate para\n",
        "    lr = 1e-4\n",
        "    lr_decay = 2\n",
        "    weight_decay = 1e-4\n",
        "\n",
        "    stage = 0\n",
        "    start_epoch = 0\n",
        "    stage_epochs = [30, 30, 30, 10]\n",
        "    total_epochs = sum(stage_epochs)\n",
        "    writer_dir = os.path.join(checkpoint_dir, \"mel_arlnet50\")\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "\n",
        "    if not os.path.exists(writer_dir):\n",
        "        os.makedirs(writer_dir)\n",
        "\n",
        "    writer = SummaryWriter(writer_dir)\n",
        "\n",
        "    train_transforms = transforms.Compose([\n",
        "        # transforms.Resize((224, 224)),\n",
        "        transforms.RandomRotation((-10, 10)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.7079057, 0.59156483, 0.54687315],\n",
        "                             std=[0.09372108, 0.11136277, 0.12577087])\n",
        "    ])\n",
        "\n",
        "    val_transforms = argumentation_val()\n",
        "    # training dataset\n",
        "    train_dataset = ISICDataset(path=data_dir, mode=\"training\", crop=None, transform=train_transforms, task=\"mel\")\n",
        "    val_dataset = ISICDataset(path=data_dir, mode=\"validation\", crop=None, transform=val_transforms, task=\"mel\")\n",
        "\n",
        "    train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "    val_loader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
        "    # get the model\n",
        "    model = arlnet50(pretrained=True)\n",
        "\n",
        "    # the loss function\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    # the optimizer\n",
        "    optimizer = optim.SGD(model.parameters(), lr, weight_decay=weight_decay, momentum=0.9)\n",
        "\n",
        "    # initialize the accuracy\n",
        "    acc = 0.0\n",
        "    for epoch in tqdm(range(start_epoch, total_epochs)):\n",
        "\n",
        "        train_acc, train_loss = train(model, train_loader, criterion, optimizer)\n",
        "        val_acc, val_loss = validation(model, val_loader, criterion)\n",
        "        writer.add_scalar(\"train acc\", train_acc, epoch)\n",
        "        writer.add_scalar(\"train loss\", train_loss, epoch)\n",
        "        writer.add_scalar(\"val accuracy\", val_acc, epoch)\n",
        "        writer.add_scalar(\"val loss\", val_loss, epoch)\n",
        "\n",
        "        if val_acc > acc or val_acc == acc:\n",
        "            acc = val_acc\n",
        "            print(\"save the checkpoint, the accuracy of validation is {}\".format(acc))\n",
        "            save_checkpoint()\n",
        "\n",
        "        if (epoch + 1) % 50 == 0:\n",
        "            torch.save(model.state_dict(), \"/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/checkpoint/mel_arlnet50/mel_arlnet50_b32_epoches_{}.pkl\".format(epoch + 1))\n",
        "\n",
        "        if (epoch + 1) in np.cumsum(stage_epochs)[:-1]:\n",
        "            stage += 1\n",
        "            optimizer = adjust_learning_rate()\n",
        "            print('Step into next stage')\n",
        "\n",
        "        if (epoch + 1) == 50:\n",
        "            torch.save(model.state_dict(), \"/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/checkpoint/mel_arlnet50/mel_arlnet50_b32_epoches_{}.pkl\".format(epoch + 1))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKE0MNAjaVCW",
        "cellView": "form"
      },
      "source": [
        "#@title dataset2017.py { form-width: \"50px\" }\n",
        "from torch.utils import data\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "class ISICDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, path, mode=\"training\", crop=None, transform=None, task=None):\n",
        "        self.path = path\n",
        "        self.mode = mode\n",
        "        self.samples = self.make_dataset(path)\n",
        "        self.crop = crop\n",
        "        self.transform = transform\n",
        "        self.task = task\n",
        "        self.image_list = []\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, melanoma, seborrheic_keratosis = self.samples[idx]\n",
        "        img_name = img_path.split(\"/\")[-1]\n",
        "        image = self.pil_loader(img_path)\n",
        "        if self.crop:\n",
        "            image = self.crop(image)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.task==\"mel\":\n",
        "            return image, torch.from_numpy(np.array(int(melanoma))), img_name\n",
        "        elif self.task==\"sk\":\n",
        "            return image, torch.from_numpy(np.array(int(seborrheic_keratosis))), img_name\n",
        "        else:\n",
        "            return image, torch.FloatTensor([torch.from_numpy(np.array(int(melanoma))), torch.from_numpy(np.array(int(seborrheic_keratosis)))]), img_name\n",
        "        \n",
        "\n",
        "    def pil_loader(self, path):\n",
        "        with open(path, 'rb') as f:\n",
        "            img = Image.open(f)\n",
        "            return img.convert('RGB')\n",
        "\n",
        "    def make_dataset(self, dir):\n",
        "        images = []\n",
        "        if self.mode == \"training\":\n",
        "            img_dir = os.path.join(dir, \"ISIC-2017_Training_Data_Patch_2\")\n",
        "            csv_filename = os.path.join(dir, \"ISIC-2017_Training_Part3_GroundTruth_patch_2.csv\")\n",
        "        if self.mode == \"validation\":\n",
        "            img_dir = os.path.join(dir, \"After-Enhancement-Val-2\")\n",
        "            csv_filename = os.path.join(dir, \"validation_gt.csv\")\n",
        "        if self.mode == \"testing\":\n",
        "            img_dir = os.path.join(dir, \"After-Enhancement-Test\")\n",
        "            csv_filename = os.path.join(dir, \"ISIC-2017_Test_v2_Part3_GroundTruth.csv\")\n",
        "        label_list = pd.read_csv(csv_filename)\n",
        "\n",
        "        for index, row in label_list.iterrows():\n",
        "            if self.mode == \"training\":\n",
        "                images.append((os.path.join(img_dir, row[\"image_id\"] + \".png\"), row[\"melanoma\"], row[\"seborrheic_keratosis\"]))\n",
        "            else:\n",
        "                images.append((os.path.join(img_dir, row[\"image_id\"] + \".jpg\"), row[\"melanoma\"], row[\"seborrheic_keratosis\"]))\n",
        "        return images"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_ku4bG5arWv",
        "cellView": "form"
      },
      "source": [
        "#@title crop_transform.py { form-width: \"50px\" }\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob as gb\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import warnings\n",
        "from scipy import ndimage\n",
        "import cv2\n",
        "def rescale_crop(image, scale, num, mode):\n",
        "    image_list = []\n",
        "    h, w = image.size\n",
        "    if mode==\"train\":\n",
        "        trans = transforms.Compose([\n",
        "        transforms.CenterCrop((int(h * scale) + 500 * scale, int(w * scale) + 500 * scale)),\n",
        "        transforms.RandomCrop((int(h * scale), int(w * scale))),\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.RandomRotation((-10,10)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "    ])\n",
        "    elif mode==\"val\":\n",
        "        trans = transforms.Compose([\n",
        "            transforms.CenterCrop((int(h * scale) + 500 * scale, int(w * scale) + 500 * scale)),\n",
        "            transforms.RandomCrop((int(h * scale), int(w * scale))),\n",
        "            transforms.Resize((224, 224)),\n",
        "        ])\n",
        "    for i in range(num):\n",
        "        img = trans(image)\n",
        "        image_list.append(img)\n",
        "    return image_list\n",
        "\n",
        "def crop(image, mode):\n",
        "    image_list = []\n",
        "    if mode==\"train\":\n",
        "        trans = transforms.Compose([\n",
        "\n",
        "        transforms.RandomRotation((-10, 10)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.Resize((224, 224)),  #change the order\n",
        "    ])\n",
        "    elif mode==\"val\":\n",
        "        trans = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "        ])\n",
        "    img = trans(image)\n",
        "    image_list.append(img)\n",
        "    return image_list\n",
        "\n",
        "class argumentation(object):\n",
        "    def __call__(self, image):\n",
        "        image_list1 = rescale_crop(image, 0.2, 15, \"train\")\n",
        "        image_list2 = rescale_crop(image, 0.4, 15, \"train\")\n",
        "        image_list3 = rescale_crop(image, 0.6, 15, \"train\")\n",
        "        image_list4 = rescale_crop(image, 0.8, 15, \"train\")\n",
        "        image_list5 = crop(image, \"train\")\n",
        "        image_list = image_list1 + image_list2 + image_list3 + image_list4 + image_list5\n",
        "        nomalize = transforms.Lambda(lambda crops: torch.stack([transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.7079057, 0.59156483, 0.54687315],\n",
        "                             std=[0.09372108, 0.11136277, 0.12577087])])(crop) for crop in crops]))\n",
        "        random.shuffle(image_list)\n",
        "        image_list = nomalize(image_list)\n",
        "        return image_list\n",
        "\n",
        "class argumentation_val(object):\n",
        "    def __call__(self, image):\n",
        "        image_list1 = rescale_crop(image, 0.2, 2, \"val\")\n",
        "        image_list2 = rescale_crop(image, 0.4, 2, \"val\")\n",
        "        image_list3 = rescale_crop(image, 0.6, 2, \"val\")\n",
        "        image_list4 = rescale_crop(image, 0.8, 2, \"val\")\n",
        "        image_list5 = crop(image, \"val\")\n",
        "        image_list = image_list1 + image_list2 + image_list3 + image_list4 + image_list5\n",
        "        nomalize = transforms.Lambda(lambda crops: torch.stack([transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.7079057, 0.59156483, 0.54687315],\n",
        "                             std=[0.09372108, 0.11136277, 0.12577087])])(crop) for crop in crops]))\n",
        "        image_list = nomalize(image_list)\n",
        "        return image_list"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmvWX1nea1o4",
        "cellView": "form",
        "outputId": "b766a56a-898a-41c0-d082-428985471bc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title generate_patch_images.py { form-width: \"50px\" }\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import glob as gb\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "def get_images(root_path):\n",
        "    files = []\n",
        "    for ext in ['jpg']:\n",
        "        files.extend(gb.glob(os.path.join(root_path, '*.{}'.format(ext))))\n",
        "    return files\n",
        "\n",
        "def rescale_crop(image, scale, num, ori=False):\n",
        "    image_list = []\n",
        "    h, w = image.size\n",
        "    if ori:\n",
        "        trans = transforms.Resize((224,224))\n",
        "    else:\n",
        "        trans = transforms.Compose([\n",
        "        transforms.CenterCrop((int(h * scale) + 500 * scale, int(w * scale) + 500 * scale)),\n",
        "        transforms.RandomCrop((int(h * scale), int(w * scale))),\n",
        "        transforms.Resize((224,224))\n",
        "    ])\n",
        "    for i in range(num):\n",
        "        img = trans(image)\n",
        "        image_list.append(img)\n",
        "    return image_list\n",
        "\n",
        "data_dir = \"/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/ISIC-2017_Training_Data/\"\n",
        "new_data_dir = base_path +'ISIC-2017_Training_Data_Patch_2/'\n",
        "excel_dir = base_path + \"ISIC-2017_Training_Part3_GroundTruth.csv\"\n",
        "new_excel_dir = base_path + \"ISIC-2017_Training_Part3_GroundTruth_patch_2.csv\"\n",
        "images = get_images(data_dir)\n",
        "\n",
        "if not os.path.exists(new_data_dir):\n",
        "    os.makedirs(new_data_dir)\n",
        "\n",
        "\n",
        "ids = []\n",
        "mels = []\n",
        "sks = []\n",
        "for img in tqdm(images):\n",
        "    image = Image.open(img)\n",
        "    labels = pd.read_csv(excel_dir)\n",
        "    mel = int(labels.loc[labels['image_id'] == img[img.rfind('/')+1:-4]]['melanoma'].values.squeeze())\n",
        "    sk = int(labels.loc[labels['image_id'] == img[img.rfind('/')+1:-4]]['seborrheic_keratosis'].values.squeeze())\n",
        "    image_list1 = rescale_crop(image, 0.2, 15)\n",
        "    image_list2 = rescale_crop(image, 0.4, 15)\n",
        "    image_list3 = rescale_crop(image, 0.6, 15)\n",
        "    image_list4 = rescale_crop(image, 0.8, 15)\n",
        "    image_list5 = rescale_crop(image, 1, 1, True)\n",
        "    image_list_all = image_list1 + image_list2 + image_list3 + image_list4 + image_list5\n",
        "\n",
        "    for i in range(len(image_list_all)):\n",
        "        new_name = img[img.rfind('/')+1:-4] + '_' + str(i) + '.png'\n",
        "        new_dir = os.path.join(new_data_dir, new_name)\n",
        "        image_list_all[i].save(new_dir)\n",
        "        labels = pd.read_csv(excel_dir)\n",
        "        ids.append(new_name[:-4])\n",
        "        mels.append(mel)\n",
        "        sks.append(sk)\n",
        "data_frame = pd.DataFrame({\"image_id\": ids, \"melanoma\": mels, \"seborrheic_keratosis\": sks})\n",
        "data_frame.to_csv(new_excel_dir, index=False, sep=\",\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2000/2000 [2:13:06<00:00,  3.99s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlMUvkb2a91b",
        "cellView": "form",
        "outputId": "d2cafa47-07b0-49cd-fd2a-cfcbc894a17e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "#@title predict_2017.py { form-width: \"50px\" }\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "# from dataset2017 import ISICDataset\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "# from crop_transform import *\n",
        "# from models.ARL import arlnet50\n",
        "RANDOM_SEED = 6666\n",
        "\n",
        "\n",
        "def main():\n",
        "    np.random.seed(RANDOM_SEED)\n",
        "    torch.manual_seed(RANDOM_SEED)\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "    random.seed(RANDOM_SEED)\n",
        "\n",
        "    # set the parameters\n",
        "    checkpoint_dir = base_path + \"checkpoint/mel_arlnet50_b32_best_acc_second.pkl\"\n",
        "    result_dir = base_path + \"result\"\n",
        "    data_dir = base_path\n",
        "    # Create the dataloaders\n",
        "    batch_size = 1\n",
        "\n",
        "    y = []\n",
        "    y_score = []\n",
        "\n",
        "    if not os.path.exists(result_dir):\n",
        "        os.makedirs(result_dir)\n",
        "\n",
        "    def imshow(y_pre, y_score):\n",
        "        fpr, tpr, thresholds = metrics.roc_curve(y_pre, y_score)\n",
        "        auc = metrics.auc(fpr, tpr)\n",
        "        print(auc)\n",
        "\n",
        "        plt.plot(fpr, tpr, c='r', lw=2, alpha=0.7, label=u'AUC=%.3f' % auc)\n",
        "        plt.plot((0, 1), (0, 1), c='#808080', lw=1, ls='--', alpha=0.7)\n",
        "        plt.xlim((-0.01, 1.02))\n",
        "        plt.ylim((-0.01, 1.02))\n",
        "        plt.xticks(np.arange(0, 1.1, 0.1))\n",
        "        plt.yticks(np.arange(0, 1.1, 0.1))\n",
        "        plt.xlabel('False Positive Rate', fontsize=13)\n",
        "        plt.ylabel('True Positive Rate', fontsize=13)\n",
        "        plt.grid(b=True, ls=':')\n",
        "        plt.legend(loc='lower right', fancybox=True, framealpha=0.8, fontsize=12)\n",
        "        plt.title(u'ROC and AUC for ISBI2017', fontsize=17)\n",
        "        plt.savefig(\"/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/ISIC-2017_Training_Data/2017_mel_arlnet50_e100_b32_second.png\")\n",
        "\n",
        "\n",
        "    def load_checkpoint(checkpoint_path):\n",
        "\n",
        "        # Here put the pretrained model that you used (in my case it's densenet161).\n",
        "\n",
        "        # model = resnet50()\n",
        "        # # model = danet()\n",
        "        # # model = resnet50_cbam(pretrained=False)\n",
        "        # # model = se_resnet50()\n",
        "        # # model = proposed()\n",
        "        # # model = models.resnext50_32x4d(pretrained=False)\n",
        "        # try:\n",
        "        #     n_ftrs = model.classifier.in_features\n",
        "        #     model.classifier = classifier(n_ftrs)\n",
        "        # except AttributeError:\n",
        "        #     n_ftrs = model.fc.in_features\n",
        "        #     model.fc = classifier(n_ftrs)\n",
        "        # model = model.to(device)\n",
        "        '''\n",
        "        fn1 = FeatureNet_1()\n",
        "        fn2 = FeatureNet_2()\n",
        "        cfn = ClassifierNet(fn1, fn2) \n",
        "        model = cfn\n",
        "        '''\n",
        "        model = arlnet50(pretrained=True)\n",
        "        # checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cuda')\n",
        "        '''\n",
        "        from collections import OrderedDict\n",
        "        new_state_dict = OrderedDict()\n",
        "        for k, v in checkpoint.items():\n",
        "            name = k[7:]  # remove module.\n",
        "            new_state_dict[name] = v\n",
        "        '''\n",
        "        model.load_state_dict(checkpoint)  # your checkpoint's key may differ (e.g.'state_dict')\n",
        "        model.eval()\n",
        "        return model\n",
        "\n",
        "    def predict(model, dataloader):\n",
        "        mel_tn = 0\n",
        "        mel_fp = 0\n",
        "        mel_tp = 0\n",
        "        mel_fn = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for ii, (images, labels, _) in tqdm(enumerate(dataloader, start=1)):\n",
        "                images = images.to(device)\n",
        "                scores = []\n",
        "\n",
        "                for i in range(len(images[0])):\n",
        "                    pred = model(images[:, i])\n",
        "                    scores.append(pred)\n",
        "                scores = sum(scores) / len(scores)\n",
        "                logps = F.logsigmoid(scores)\n",
        "                score = torch.exp(logps)\n",
        "                pre = torch.ge(score, 0.5).float()\n",
        "                if int(pre) == 0 and int(labels) == 0:\n",
        "                    mel_tn += 1\n",
        "                elif int(pre) == 1 and int(labels) == 0:\n",
        "                    mel_fp += 1\n",
        "                elif int(pre) == 1 and int(labels) == 1:\n",
        "                    mel_tp += 1\n",
        "                elif int(pre) == 0 and int(labels) == 1:\n",
        "                    mel_fn += 1\n",
        "                score = score.cpu().numpy().tolist()[0]\n",
        "                label = labels.cpu().numpy().tolist()[0]\n",
        "                y.append(label)\n",
        "                y_score.append(score)\n",
        "        return mel_tp, mel_tn, mel_fp, mel_fn\n",
        "\n",
        "    val_transforms = argumentation_val()\n",
        "    # Validation dataset\n",
        "    val_dataset = ISICDataset(path=data_dir, mode=\"testing\", crop=None, transform=val_transforms, task=\"mel\")\n",
        "    val_loader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    model = load_checkpoint(checkpoint_path=checkpoint_dir)\n",
        "\n",
        "    model.cuda()\n",
        "    mel_tp, mel_tn, mel_fp, mel_fn = predict(model, val_loader)\n",
        "\n",
        "    mel_acc = (mel_tp + mel_tn) / (mel_tn + mel_fp + mel_tp + mel_fn)\n",
        "    mel_sen = mel_tp / (mel_tp + mel_fn)\n",
        "    mel_spe = mel_tn / (mel_tn + mel_fp)\n",
        "\n",
        "    y_score = np.array(y_score)\n",
        "    mel_auc = metrics.roc_auc_score(y, y_score)\n",
        "\n",
        "    imshow(y, y_score)\n",
        "\n",
        "    print('mel_Accuracy:', mel_acc)\n",
        "    print('mel_Sensitive:', mel_sen)\n",
        "    print('mel_Specificity:', mel_spe)\n",
        "    print('mel_AUC:', mel_auc)\n",
        "    with open('/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/result/result_second.txt', 'a') as f:\n",
        "        f.write('\\n2017_mel_arlnet50_e100_b32: ' + json.dumps(\n",
        "            {'mel_Accuracy': mel_acc, 'mel_Sensitive': mel_sen, 'mel_Specificity': mel_spe, 'mel_AUC': mel_auc}))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "600it [03:23,  2.95it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.7467218771566599\n",
            "mel_Accuracy: 0.8183333333333334\n",
            "mel_Sensitive: 0.1111111111111111\n",
            "mel_Specificity: 0.989648033126294\n",
            "mel_AUC: 0.7467218771566599\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEdCAYAAAAIIcBlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOy9eZgc1Xnv/3lnNNp3hCSkQUIWAiMEWpHYxCYWCYPtxMQBDA5OCMmNcfwzjm3s5NpAAubm2onhhuTaxmaxY2PsG2MIAtsYkIRAy2hntO/SaDSjZUbDaNbufn9/VLXoafVS3VVd58xMfZ+nnp6uPnW+n3O6pk+dXVSVSJEiRYoUKaky0wCRIkWKFMkuRQVDpEiRIkXqoqhgiBQpUqRIXRQVDJEiRYoUqYuigiFSpEiRInVRVDBEihQpUqQuigqGSN1OIvK2iLxtmiMoicgMEVkiIk0ioiLySdNMkXq3ooLBYonIPe4PRfKIi8hhEfmFiJyf47pZIvKCiBwSkQ4RqXWvmZPjmgoR+SsRWSoix93rDojIz0Xk2tKkMDyJyMtuHj6Z5fNr3M/vyvL5Xe7n12T47AIReVpEdotIm4h8ICKrROQbIjI8D1c58CvgbOBrwN1AVaHpK0Ruwboz7VyFiPy1iKwWkQYRaRaRne59tDAtrKYdrSKyQ0S+IyKj0sIm7+ErU85d5+bXVhFpEZF97v15XhbesSLyUxE55nK9KSKzM4S7UUR+KCIbRCQmIhknaaV819mO3xeSnz1RfUwDRPKkfwS2A32B6cB9wHUiMk1V61IDisi9wPeBQ8CPgD3AROBzwEoR+StVfTrtmhHAfwOXA78HHgUacH6s/gh4U0SuUNV3S5fE0sn9sVqIkxd/KiIPqGosoLjvAn4MNAI/BbbgfE/zgH8ArgFuzBHFJGAy8CVV/Y8gmIrUi8AngP8H/ASIAecC1wO3A6+nhX8b5/4CGADMBr4ILBCRWZp75uw/A2e4XluBccDngfXufbYuGVBEBgFvAWOA7wIn3LBvichcVd2aEu+dLusGnO/63Cz+W3AK4HRdC/x5hrT2PqlqdFh6APcAClyZdv6v3fNfSTs/F+cfegUwNO2zIe75TmBu2me/BhLAZ3JwzPWTloDz5W3g7QLC3w+0A/PdfPtYhjDXuJ/dlSWOu9zPr0k5NwfoAFYCIzJcMw74Zh62y9147wkwfyqAvnnyb2daOhR4LEv4MWnvFXg6Q7j/7X52Ua57GLgKKEu79lygFfivtPNfdq+/NuXcmTgPLr/KkN993b+fdn7eCsq3V9z/n7Gm7m1bjqgpqXtqmfua/kT0LUCAu1W1KfUDVf0A+CxO8+G3kufd5qVPAs+p6n9mMlPVZ1V1VS4gt8ng926zVYfbrPJtEemXFu5Zt5o/RkRedNvVG9ymhQFpYUVEvuo2NbSKyHsicnkujiy6G3hdVZcBm8j8tFiMHsLJzztUtSH9Q1U9pKqPZLtYRJ4Flrtvn3GbMfamfH6hiPxGRBrdJpcVInJLWhzJZpHPisjfu9e3AVMLSEfyPlqS6UNNq5Xm0GH3NWdtTFWXqmoi7dxOYCOnc38aqFbVt1LCHsGp4dwiIgNTzh9S1Q6PrF0kImfi1Cp/r6qH84Xv6YqakrqnznFfjydPuD+qNwDvqOqOTBep6nYRWQ5cLyIDVLUVp1AAeNYn0+eBbcBvgWacJ+GvAhOAz6SFFZzq+macdvVLgL8AjgBfTwn3TZwf3z/gPI1OwWnyagAOeIFy263n4jQxAPwM+KaIDE0vPAuRm983AstUdXeR0Xwf2A/8T+AHOAV+cwr3uzg1vO8BTThP3y+LyJ+q6i/T4voqTiH1FM4P83G8a6/7epeIvK2q7R6u6ZfSnzAAmAk8gFN72pr1qiwSEQHGAgdTzpXhNJ3+LMMlq3CaVC8EVhfql0F34PwePh9AXN1fpqss0ZH94MNq+MeAUThV5UXADiAOzE4Je7Eb9ok8cT5JSnUf+C/3/WlNIQWyDsxw7h9wmqgqU8496/o9nhb2JeBIyvtROM0/bwLlKefvc69/2yPXP+L8qA5w309wmf48Ldw1FNCU5DW/PfBdSYamJJwO6RhwYcq5IcBuoAbok8ZdAwzx6Pk2XZuSBKdvSYGjOG3/XwamZrlesxzLgVFZ7uEr8zAl8/cLafeAAo9mCH+z+9knssRXUFMSTuFy6j7p7UfUlNQ99N84T9M1wGJgEHCnqq5JCTPUfc33FJz8fGjaa9FPzwCq2gLOU56IDHefJpfi/OjMynDJv6e9XwKMEpEh7vsbcDpx/4+qxlPCPYPTAZlX7lPoXcBL6tSOUNX9OD9gfpuTAsm3THJHKi0EFqtqdfK8Os2B/4HzgJCepz9xPy9Y6vwyfhz4BlAP/DHwHaBaRN4VkSkZLnsN5zu6AbgV5yFgKvCb9CbBfBKRC3Huhyqc9CWVjCdTDaYtLUzREpELcPpZfpW8T3q7ooKhe+hLOP+Afwy8AIzAaWJIVfoPfjYlP0/+iCSvG5IhrGeJyKUi8ibQgtPUc4QP26zTh2wmSGkycJVsox/pvk50X7elBlLVTpynZi+6EqfZbbmInJs8cJ6YrxaRCR7j6YLgvgaSb1l0Jk7hn6lJZrP7Oint/C4/hqraqqrfVtWpOCOGPgb8ErgMeCW9rwg4pKpvuMd/q+qjwL04TYj3evUVkbNxCplG4JPadbRY8kc63Rugf1oYP0o+JETNSK6igqF7qMr9B/y1qt4B/A54VkTGp4TZiVNYzMgT13Q3XLIfYov7enGxcCIyCafJ50ycJohbcQqye9wg6feZalrnY2p0xXJkUPIf/v/ipDd5/IPrk9r3ke8JNNnJmfwhSuZ30fkWsAJ70lXV46q6WFU/jTN09Xyc4bf59Af39SovPm6H7+9w8vxGVa1JC3Icp7YwLsPlZ7mvh7x45WBI3gf7yNL53hsVFQzdU1/FeaL8n8kTblPOH4ArRWRypovcp+UrcUZeJH9IfuO+/pkPno/j/HPfoqpPuU+QbwC1PuLc5752mcgnIhWc/rR8mtwn3D/BaYb7kwzHO3RtTtrrvn40S5QXpHK5+f0GcJVbMAapI8DJLCxJjj0Be2ZTcjRaph/ndCUHswzOF1BEhuEMVBgPLNKu8xEAcB8eNuAMTkjXPJxCY3OGzwrRNTj9Tj91m9QiERUM3VKqug1n7sHn0moNj+A0dfwkpa0eABEZjFNVVjdcMq5VOOO3Pycit5NB7lDIuTmQkk//p+4nd0TJlz0n6nT9HmeOwBfcuJL6HKc3TWXSrW64/6uqv0o/gOeAC5IzaNUZorgWuMN9kj0lERmNM2pljXYduvkwTn7+p2SY4SwiZ4nI/0w/n09un8prwCK3/TsZ32Dgf+A8Ja8tNN5sEpEpmQo392n6Zvetl5FGt7qvG/L4DcTpK7sA+Liq5prp/SvgQkmZce5+P3+C0wdz0gNXLn3WfY2akVIUDVftvvpn4FPA3+H0QaCq74nI/ThDFqtF5BmcJ9wJOD+olcBfq+rKtLjuwflH/bmIfA7nSa7BDf9JnI7OXPMHXsd5entVRL6PU0B8msxtw56kqkdF5H/h1Ip+JyIv4Yy3/yze+hjuxnnq/kOWz1/B+VG/G0h24n8RpxawUUR+hDOUdAJOm/kw4LY0xpXizDR/GtgmIqkzn+fg5MFyitPf4wyHXSoi/8aHw1UnAX+qAc3cdjUdeEGcpSCWAHU4I4L+CKeP4Zequj7tmsny4fIh/XCaMP8Sp7aTcdmRFP0nzv30c6BS0pYhUdWfprz9D5z8/y8R+Q4fznzug9MkeEoicjFO7RWXBxFJhtmnqj9JCz8A539opapuz8Pcu2R6WFR0ZD/IM9QPZ6mAk5w+RPAS4Bc4TTkdOBOPXgTm5PDqi/M0ugynI7ADZ67Az4D5HlhvxBlV0uL6/RswjbShmDjDVWM50npOyjkBHnQ5WnFmbl9OnpnPOJ2nHcCv8zC/h/Mj2Cfl3Eyc4Zr1OH0I9e77mTniuRBneYi9OAXkBzhNMF8jbQZ6hmszDldNifdlnB/DZPpvSQtzDTmG2WbxfJuuw1VHA1/BKUQPunnXhDMn4Yup+eOGTx+mGnfvtZ8Ak/Ldw24+ZRvyqhl4x7n34XGc+/2tTPdyilem47T7BacWqMDfmPoft/UQN4MiRYoUKVIkIOpjiBQpUqRIaYoKhkiRIkWK1EVRwRApUqRIkbooKhgiRYoUKVIXdevhqqNGjdJzzjnHNEakSJEidSutWbPmqKqeme3zUAoGEfkxcAtQr6rTMnwuwBM4k2lacIbu5Z3Ac84551BV5cyN2bVrF5MnZ5zwG5pMM5j2t4HBtL8NDKb9bWAw7W8DQy5/EdmX8QNXYTUlPYuzWmQ2LcJZa38KzrLKBW9xOHLkyPyBSizTDKb9bWAw7W8Dg2l/GxhM+9vA4Mc/lIJBVZeSe+OQTwDPq6MVwHAROStH+NPU0tLiBzEQmWYw7W8Dg2l/GxhM+9vAYNrfBgY//rb0MYyn645cB91znhdhKysz349umsG0vw0Mpv1tYDDtbwODaf+SMDz8MFTlWlbqQx3p35+2eBz+67+KsjKfewVKRO4TkSoRqaqtreXo0aPU1tbS2NhIQ0MDu3btorW1lc2bN5NIJFi71umqWLPGWQ5n7dq1JBIJNm/eTGtrK7t27aKhoYGamhqS8e3du5fm5ma2bt1KLBZjw4YNXeJIvm7atIn29nZ27NhBU1MTDQ0N1NfXU19fz/79+2lqamLHjh20t7ezadOmjHFs2LCBWCzG1q1baW5uZu/evafSVFNTU1CaampqAk/T/v37C0pTRUVFoGkq9HuqqKgIPE2Ffk81NTWh33upaWpoaAj93ktPU2NjY+j3XmqaamtrQ7/3Mv0vBJWm+Le+RfPbbxOLxWhpaaGjo4O2tjZa29ro6Ozk5MmTxONxDnZ2cqRfP/rU1zOioyNrmvIptCUxROQc4L+zdD5/H2ctk5+777fhbKGYs8YwZ84cTXY+7927F9MjlEwzmPa3gcG0vw0Mpv1tYDDtHzjDre7CtXPmwLe+ddrH8Xic6upqdu7cyZw5c5gwYUJOfxFZo6pzstnZ0pT0MnC/iLyAs876iXyFQrpGjRqVP1CJZZrBtL8NDKb9bWAw7W8Dg2l/TwwFNA2dUoZCAeDdd98lkUiwcOFCBg4c6M0/h0JpShKRn+OsZHm+iBwUkb8Qkb8Wkb92gyzGWUp5J/BD4G8K9Th4MH2nyPBlmsG0vw0Mpv1tYDDtbwODaX9PDIUWCnO6PuDHYjGqq6uJx+PMnTuXq6666lSh4Mk/h7r16qqpTUmxWIw+fcxWgEwzmPa3gcG0vw0Mpv1tYAjMv5inelcJVcrEw061r7xScNyHDx9m1apVjBo1ijlz5tC3b9/TwuTKg3xNSd2u8zmbqqurTSMYZzDtbwODaX8bGEz728AQmH+RhQLABx98kD/QnKy/zVnV1NTEypUrmT17NpdffnnGQgH85UGPqTFEihSpl8rHU71nFfFUH7QOHjzIyZMnOf/884nH45SXlxcdV6+pMSSHZfVmBtP+NjCY9reBwbR/6AwZCoXGEyeCi7+Ip3oILg/a2tpYvnw569atY8SIEQCeCgU//lGNIVKkSN1XqbUFC57qS6G1a9dSVlbGRRdd5KuWkKqoxtCLGEz728Bg2t8GBtP+gTM8/LAzjj/TkSwU0p7qu3setLS0sHTpUpqampg5cyYzZswouFCIagyRIkXquUpO7sqmLJO+uqNUlZ07d7Jp0ybOO+88pk6dWpLlPXpNjSE5nbw3M5j2t4HBtL8NDKb9fTOk1xCSeuWVzEeGQqE75oGq0tnZyaFDh1iwYAHTpk3zVSj4yYMeU2Nob2+nX79+RnlMM5j2t4HBtL8NDKb9fTNkqiEUWCvoTnmQSCTYunUrx44dY/78+aH495oaw/79+00jGGcw7W8Dg2l/GxhM+xfNkKwpJJWnVhC4f8DywtDQ0MDvfvc7Dh8+zMyZM0P3zyZb1kryrTFjxphGMM5g2t8GBtP+NjCY9i+aIbW/sMghor78A1Yuhng8TllZGR988AFTpkzhIx/5COJllnRA/vnUYwqGxsZGhg4d2qsZTPvbwGDa3wYG0/6nMRQ6AS2AYafW5UGKjhw5cmrm8oQJE0L396IeUzD079/fNIJxBtP+NjCY9reBwbT/aQyFFAo+awoZ/Q0pnSGRSLBu3ToOHDjArFmzGDt2bKj+hajHFAyRIkWyQG7tYFh7O6R3fPbQCWhe1NbWRr9+/Rg4cCCLFi0y3jGeT6EVDCKyEHgCKAeeVtXH0z6fCPwYOBNnf+i7VNXzurFtbW0B0hYn0wym/W1gMO1vA0Ng/n5WFk0kup4IqCbgVaa/gyRDR0cHa9eupaGhgYULF3LBBReE6l+sQikYRKQceAq4AWc/59Ui8rKqbk4J9h3geVV9TkSuA74N3O3VY/jw4UEiFyXTDKb9bWAw7W8DQ2D+xU4enTOHzi99iQEG2/hNfwfgLHu9ePFiKisruf766wPvXM4nP3kQVo1hLrBTVXcDuDu1fQJILRimAg+4f78FvFSIQV1dnfHOJtMMpv1tYDDtbwND4P5FNAHV7djRs/KgALW2tlJWVkZzczOXX345o0ePNsLhJw/CmscwHjiQ8v6gey5VG4A/dv/+I2CIiJyRHpGI3CciVSJSldyYu7a2loqKilA3+obTNy8vLy8v6Sbz+dLU2tpa0k3mvaRpwoQJJd1kPl+aJkyYUNJN5r2kqbW1NfR7LzVN5eXlgaQpucl8Md9TRUVF6Pdeapo6OjpCv/dUld/+9re89tprLF26lPPPP5+6urpQ773UNKX+L6SnKZ9CmfksIrcBC1X1Xvf93cA8Vb0/Jcw44N+AScBS4FPANFVtzBZv6sznTZs2cdFFF5UuER5kmsG0vw0Mpv1tYCjI30s/QhE1hm6VBwFIVVm2bBknT55k3rx5jBw50uo8yDfzOayC4TLgIVW9yX3/dQBV/XaW8IOBrapamSveaBG9SJF8qhctUFcKqSp1dXWMHTuWo0ePMnLkyJIsehe08hUMYfUxrAamiMgkoAa4HbgzNYCIjAKOq2oC+DrOCCXPWrNmDbNnzw4ItziZZjDtbwODaX8bGLr4ex1ZFPBQUqvyoERKbrEJcOaZZzJq1KjQGXLJj39oi+iJyM3A93CGq/5YVR8VkUeAKlV92W1u+jagOE1Jn1fV9lxxRjWGSJHyKF+NAKJaQRE6cuQIS5cu5aKLLmLKlCmhjzjyq3w1BlS12x6zZ8/WpKqqqtS0TDOY9reBwbS/DQyn/B96SPWWW5zDFIMhlcr/+PHjeuTIEY3H49rc3GyEwaty+eM8kGf9be0xy25HihTJVXrzUVQj8K14PM6mTZvYvXs3l1xyCWeffbZpJF/qNctuJ4fA9WYG0/42MJj2t4HhxJtvfvjGUKFgOg+C9l++fDnNzc0sWrTIc6HQnfOgx9QYYrEYffqYXfrJNINpfxsYTPuXlMFjR3JClTIRo2sTmf4egvDv7Oxk69atTJ06lXg8Tt++fUNn8KNc/r2mxrBz507TCMYZTPvbwGDav6QMHptNW06eDH1tonSZ/h78+tfW1rJ48WJOnjxJIpEouFAIgsGv/Pj3mNVVKytzTnnoFQym/W1gMO0fCkO+mkBzMwweXFqGPDL9Pfjxb2pqYvXq1cydO5ezzjrLCEMQ8uPfY2oMR48eNY1gnMG0vw0Mpv1tYDDtbwNDof6qyv79+9myZQtDhw7llltu8VUoFMMQtPz495gaw2DDT0g2MJj2t4HBtH9JGApc/rpH5kEJ/VtbW6mqquLEiRPMmzcPIJDZy90pD9LVYwqGzs5O0wjGGUz728Bg2r8kDAXuhdwj86CE/slawuWXX055ebkRhlLIj3+PKRhO2xikFzKY9reBwbR/YAyZagkeRxn1mDwoof/Jkyepqqpi5syZzJw5syQzl23Pg1zqMQXDwIEDTSMYZzDtbwODaX/fDNmajQoYZdTt86CE/qrK9u3bef/997ngggsYPHhwyZazsDUPvKjHdD4fP37cNIJxBtP+NjCY9vfNkN5s9MorzlHAJLVunwcl8ldVOjs7qa+v54YbbmDq1KklXQnVxjzwqh4zwa21tZUBAwYY5THNYNrfBgbT/lkZCt0/2cfkNGvzwJB/IpFgy5YtHDt2jKuuusoIgwnl8rdmgpuILBSRbSKyU0QezPD5BBF5S0TWichGdzVWz9qzZ09wsEXKNINpfxsYTPtnZSikUPA5Oc3aPDDgf/z4cV5//XWOHDkS+hLYtuRBMQpro55yYDtwA862nquBO1R1c0qYHwDrVPU/RGQqsFhVz8kVb2qNIZFIGN8gwzSDaX8bGEz7Z2RIrS2EsEyFlXkQsjo7O+nTpw8HDx4kFotxzjnnhL40tuk8yOVvS41hLrBTVXeragfwAvCJtDAKJHeuHgYcKsRg/fr1viH9yjSDaX8bGEz7Z2RIFgohLVNhZR6EqPr6en76059SV1fH2WefzaRJk4zsl2D6e/DjH1bBMB44kPL+oHsuVQ8Bd4nIQWAx8IVMEYnIfSJSJSJVyY25a2trGTNmTEk3+k6NI9vm5aNGjQpto+9Maerfv39JN5n3kqZZs2YFmqZCv6dZs2aVdJP5TGlq+vKX6Vi4kJPXXUf85puZ8sAD6C23cOKqq+DWW2k8ccJJ0623luzeS03TqFGjQr/30tM0ZsyY0O+9RCLBSy+9xPLly5kyZQqDBw8O9d7L9L9Q6nsvV5pS/xfS05RPYTUl3QYsVNV73fd3A/NU9f6UMA+4PN9194j+ETBNna0+Myq1Kcn0Nno2MJj2t4HBiH/aLmmNJ04wfNiwrmFCXP7a9HdggqG1tZX+/fuzbds2PvKRj7Bp06ZelweF+OdrSgqrYLgMeEhVb3Lffx1AVb+dEqYap/A44L7fDVyqqvXZ4o026okUigztmxwpv9rb21m7di0nTpzgpptu6nZbbJqSLX0Mq4EpIjJJRPoCtwMvp4XZDywAEJELgP7AEa8GyeqgSZlmMO1vA0PR/g8/7Dz5Zzq8FAop/QfdNg+6GUNdXR2LFy+mX79+XH/99V0Khd6SB6XyD20egzv89HtAOfBjVX1URB7B2Xv0ZXck0g+BwTgd0V9V1d/lijMalWSXvw0MRfunNQedpgKagrptHnQThtbWVsrKyujs7KStrY1Ro0aF6u9VphlCG5Ukjopai1ZVF6vqeao6WVUfdc99U1Vfdv/erKpXqOp0VZ2Rr1BI19atW4vBClSmGUz728BQlP/DD3/4d3KmcfpRQP9At8yDbsCgquzatYvXXnuNuro6Bg8enLFQKJV/oTLN4Mff01pJIjIYeAL4DBAHBonIJ4HpqvpwzotD0qRJk0wjGGcw7W8DQ1H+AQ8n7ZZ5YDmDqrJ06VLa2tq47rrrGD58eKj+xcg0gx9/rzWG7wJjgCuADvfcauBPi3YOWIcOFTTtoUcymPa3gSGnf7Z+hKQCGjVkdR50MwZV5dChQ4gI06ZN44YbbshbKATp70emGfz4e11d9RZgqqqeEBEFUNUaERlXtHPAGjlypGkE4wym/W1gyOmfqxM5wMlnVudBN2JobGxk1apVlJeXM2bMGM4444xQ/f3KNIMff68FQxnQZVaE27zUXLRzwGppaWHEiBG9msG0vw0MGf3Th5uWeFiplXnQzRjq6+t55513uPjii5k8eXLBw1B7Qh6Y9PdaMLwDfB1I7U/4AvBWUa4lkOkRCDYwmPa3gSGjf4E7oJWEIUSZ9vfDcOzYMRKJBKNGjWLhwoVF7ynQnfPABn+vBcMDwJsichcwWEQ2AX2B64p2DlgVFRWmEYwzmPa3gaGLf8g1hYwMBmTavxiGWCzGpk2b2Lt3L5dccgllZWW+Nprpjnlgk7+nIsWdjTwNeBD4BvAIMENVa4p2DljNzeZbtUwzmPa3gaGLf8g1hYwMBmTavxiGd999l5aWFhYtWkRlZWXo/qWQaQY//l6Hq96tqj8B/l/a+c+o6n8W7R6gso1n7k0Mpv19MxS6mU0GVcZi0Cfttg55qQrT34Npf68MHR0dbNmyhQsvvJDLLrss0Cfs7pIHtvp7bYR6Ksv5/1O0c8A6ePCgaQTjDKb9fTEEUCgAtLW1dT0RYk0hKdPfg2l/Lww1NTW89tprtLe3o6qBN7t0hzyw2d/Tkhgi8oGqDkk7dw6wSlVHF+3uU6lLYsRiMfqkPymGLNMMpv19MSTnE/hchbRb50EP8c/H0NTUxJIlS5g7dy5jxowJ3T8smWbI5e9rSQwR6RSRDmCgiHSkHsAu4EU/4EGqurraNIJxBtP+BTGkTzZLyucks26VBz3UPxODqrJv3z42b97M0KFD+djHPlayQiGTvwmZZvDjn7PGICJXA4Kzcc6ilI8SwGFV3VG0cwCKlt3uxsq0aF2IexZECk8tLS2sXr2akydPMm/evIImqkUqjXzVGFR1iaq+DUx2/04eywotFERkoYhsE5GdIvJghs//VUTWu8d2EWksJP7kTkUmZZrBtL9nhmyL1gVQKHSbPOjB/ukM27ZtY+TIkSxcuDC0QsG2POhu/p6X3RaRM4BLgDNxahEAqOrzHq4tB7YDN+Bs67kauENVN2cJ/wVgpqr+ea54oxpDN1VA/QmR7NUHH3xAVVUVs2fPZujQofkviBSqAll2W0SuB3YDPwF+jLOo3o+Af/DIMRfYqaq7VbUDeAH4RI7wdwA/9xg3YL50toHBtH9GhkwL1yVVgkLByjzoRf6JRIItW7bwwgsvcNZZZzFkyJD8F5VApr8DGxj8+Hsdrvo48Iiqngk0u6//CPxfj9ePBw6kvD/onjtNIjIRmAS8meXz+0SkSkSqkhtz19bWMnbs2FA3+obTNy8/88wzQ9voO1OaBgwYUNJN5r2kafbs2V3SFFuxgpaWFjo6Omhra6O1rY2Ozk6Of+QjJfmeZs+eHXiaCv2eBgwYEPq9l5qmM888M/R7b/PmzbS0tLB9+3Zqamq45pprGDZsGMeOHQvt3ktN06BBgwJJk5/vafbs2aHfe6lpSv1fSE9TPnkdrnoCGKmqcRFpVNXhItIP2KGqE0JKRpkAACAASURBVDxcfxvOfs73uu/vBuap6v0Zwn4NqFTVL+SLN7UpadOmTVx00UV501JKmWYw6u/OQ2hqasrcdBDSJDPT34ENDGH7x+NxqquraWxs5KqrrjLCkC7T/jYw5PLP15TkdZBtC9DPfT0mIhOABsDr0n01wNkp7yvdc5l0O/B5j/Ge0nnnnVfoJYHLNIMR/7SJaYMGDz49TIiTzEx/BzYwhOl/7NgxVqxYwZAhQ5iT8j33pjywlcGPv9empHeBT7p/vwa8DLwBvOfx+tXAFBGZJCJ9cX78X04PJCIfxSlsvMZ7Svv37y/0ksBlmsGIf9p6RLu/9z1f22L6lenvwAaGMPxjsRiqSmtrK9OmTWP+/PldFr3rDXlgO4Mff681hrv4sBD5O+DLwBDgX7xcrKoxEbkf+C1QDvxYVatF5BGgKrnvM06B8YJ6HSqVolJOlukuDKH7pw87BcY0NYXLkCbT34ENDKX2r6urY+XKlcydOzfrgnc9PQ+6A4Mff08Fg6q2pvzdBjwKICLzgXqPcSzGmSiXeu6bae8f8hJXJjU2NhofFmeaIXT/DHsl97o8sJChVP7xeJyqqioOHz7MJZdcwtixY0Nn8CrT/jYw+PHPWzC4O7WdB+xT1WPuuek4I5WuBfoX5Ryw+vc3j2GawZh/SlNRr80DixhK4d/S0sKAAQMYMWIEs2bNyrvoXU/Mg+7G4Mc/31pJ1+J0ElcBB0TkZhH5R2Cle/6jRTtHihTJerW1tbF8+XKWLVsGOB2apjegiVR65asx/BPwA+AZ4D6cCW5bgItMr5OUrtOWW+6FDKH6p/YvmGKw0N8GhqD8Dx8+zHvvvcekSZO49NJLC9p3uafkQXdm8OOfr2D4KHCNqnaKyDdw9nn+lKrWFe1YIg0fPtw0gnGGUP0z9C+EzpBBpv1tYPDr39LSQllZGUOGDOGqq64qan2j7p4HPYHBj3++4aoVqtoJoKotwAkbCwVwRkqYlmmG0PxTawtpQ1F7TR5YzFCsv6qyY8cOXn/9dY4cOcKgQYOKXvSuu+ZBT2Lw45+vxlAhInfw4aJ56e9R1Z8V7R6gJkzIOwG7xzOUzD/b7moZJq712DzoRgzF+KsqS5YsoaOjgwULFjBs2LDQGYKUaX8bGPz456sx1AGP4QxPfRQ4mvb+n4p2Dljbt283jWCcoWT+2QqFDBPXemwedCOGQvwTiQQ1NTWICBdffDE33HCD70KhUIZSyLS/DQx+/D0vu22jomW3S6z0mkJI6x1FCkcNDQ2sXLmSvn37cvXVV1NeXm4aKVJICmTZ7e4g00vc2sAQuH/achdGGAqUaX8bGLz419fX89Zbb3Heeedx7bXXBl4odIc86OkMoWzUY6OiGkOJFNUUeqyOHj1KIpFg1KhRtLe3M2DAANNIkQwoqjH0IobA/IuoKQTOUKRM+9vAkMk/FouxZs0a3nnnHTo7OykrKytpoWBjHvQ2hqjGECk4pdYWoppCj9GSJUvo27cvs2bNol+/fqZxIhmWNTUGEVkoIttEZKeIPJglzKdFZLOIVItIQcNgkzsomZRphkD8s0xcC5XBh0z728CQ9O/o6GD9+vXEYjGuuOIKLrvsstAKBVvyoDcz+PH3uoNbOfB14M+A0ao6TERuAiapat7tPd3rtwM34GzruRq4Q1U3p4SZArwIXKeqDSIyWlVzrtyaWmOIxWL06eN1FfHSyDSDZ/9s8xJSVWRtodvkQQ9miMVi1NbWsmbNGiorK5kxY0boPDbkQXQfZPcPqsbwj8DHga8ByZJkO/BXHq+fC+xU1d2q2gG8AHwiLcxfAk+pagNAvkIhXTt37iwkeElkmiGv/8MPw6235i8UfOy4Zn0e9AKGjRs3snHjRq644grmzJlj5MfJdB6Y9reBwY+/14LhTuATqvpfQMI9txc4x+P144EDKe8PuudSdR5wnogsF5EVIrIwU0Qicp+IVIlIVXJj7traWioqKkLd6BtO37y8vLw8tI2+M6Wpra0tZ5piK1bQ3NxMQpXDlZXwyiuseeihLq+bHnuM9gcfLHrz8srKysA3mS/ke6qsrCzpJvNe0tTW1hb6vbdv3z7Wrl3Le++9x5AhQ7jyyitpbGwM7d5LT1NFRYXvNPn5njo6OkK/9zL9L4R976WmKfV/IT1N+eS1KakeOEtV4yJyXFVHikg/YK+qnuXh+tuAhap6r/v+bmCeqt6fEua/gU7g0zh7Qi/FWcW1MVu8qU1Je/fu5ZxzzsmbllLKNMMp/3xNRSXsVLYmDwwqbIaTJ0+yatUq2trauPTSSzlx4kSvywPb/G1gyOWfrynJax1zLfA54OmUc3cCqzxeXwOcnfK+0j2XqoPASnfRvj0ish2YgtMfkVeDM21CH7JMM5zyz1Uo+GgmKojBkEz7m2DYsWMHo0eP5oILLqCsrIx4PB6qfyaZ/h5M+9vA4Mffa8Hwd8DbInI7MFBEXgHm4Ozg5kWrgSkiMgmnQLgdp2BJ1UvAHcAzIjIKp2lpt8f46ezs9Bq0ZDLN0O/xx2Hbtg9PGBhuajoPTPuHxdDU1MTq1au55JJLmDFjRuj++WSawbS/DQx+/L3u+fy+iEwF7ga2AvuAe70uwa2qMRG5H/gtUA78WFWrReQRoEpVX3Y/u1FENgNx4CvJrUS9KJFI5A9UYplm6LNhAyS38ytxzSCbTOeBaf9SMyQSCbZs2cK2bduYNm0aQ4YMCdXfq0wzmPa3gcGPv6eCQUQGuKOEvluskaouBhannftmyt8KPOAeBWvgwIHFogUmYwxun8Kp9W4MTkwz/T2Y9i8lg6oSi8VoamripptuYtCgQaH6FyLTDKb9bWDw4+91VFKdiPxQRC4t2qnEOn78uGkEcwxun0JnR4exmkJSpr8H0/6lYIjH42zYsIFly5bRt29fLrvssqyFQin8i5FpBtP+NjD48ffax/Bx4B7gDRE5gLMH9HM27eY2btw40wjhM6SNPip79VUwvCia6e/BtH/QDEePHmXFihUMHz6cuXPnhu5frEwzmPa3gcGPv6cag6q+rar3AGOB7wC3APtF5DdFOwesPXv2mEYInyFtsbtemQeW+QfFEIvFUFXa29uZPn06V155Jf2T/Uch+PuVaQbT/jYw+PEvahE9EZmJs3vbQlU1trtH6jyGRCJBWZnZxWJDZ7j1VufV7VPolXlgmX8QDLW1taxatYp58+YxduzY0P2DkGkG0/42MOTyD2wRPRE5Q0S+KCLrgHeABuCmQmFLpfXr15tGCIchuaxFslAI2z+PTDOY9vfDEI/HWbFiha9CwY9/kDLNYNrfBgY//l5nPv8aWASsA54FXlDVE0W7BqReuex2eoGQZe/lSN1HqkpLSwsDBw5k165dnHPOOcYXgIvUsxVUjWEHMFNVL1PV79tQKKTL9KYYoTA8/PCHf7/yinOkFAq9Ig8s9y+UobW1lXfeeYfly5cDcO655/ouFLpbHvREfxsYoo16eouStYWoltAjVFtby3vvvce5557LhRdeGPi+y5EiZVPRNQYReTLl7x9kO4IGLlbJFRJ7JEOyXyGpLIVCj86DbuLvhaG5uZm2tjaGDh3Ktddey8UXXxxoodAd8qCn+9vA4Mc/a41BRP5DVf+H+/cz2SJQ1c8V7e5TvWZUUmqhkKO20KPzoJv452JQVbZv387777/PvHnzqKysDNU/TJlmMO1vA0NJRiUlCwX3789lO3yRB6itW7eaRgieIb2mkNanUHL/ImSawbR/NgZV5a233uLAgQPccMMNJSsUsvmHLdMMpv1tYPDj76k4E5HXs5x/tWjngDVp0iTTCMEzpE1gC92/CJlmMO2fzpBIJDhw4AAiwqxZs1iwYAFDhw4Nzd+UTDOY9reBwY+/13rO5VnOe147SUQWisg2EdkpIg9m+PweETkiIuvd416vcQMcOnSokOAlUckY8tQUSu5fgEwzmPZPZTh+/Divv/46u3btIh6PM3z4cEQkNH+TMs1g2t8GBj/+OcfFiUhyz4Q+InIHkHpXT8GZ5JZXIlIOPAXcgLMhz2oReVlVN6cF/UXqrm6FaOTIkcVcFqhMM5j2t4HBtH+Soa6ujuXLlzNr1iwmTpwYSoGQ6m9aphlM+9vA4Mc/34DpR93XfsBjKecTwGHgCx595gI7VXU3gIi8AHwCSC8YilZLSwsjRowIKrpuyWDa3wYG0/719fUcPnyYadOmcfPNN3te3yhImc4DGxhM+9vA4Mc/Z1OSqk5S1UnAq8m/3WOyql6hqq959BkPHEh5f9A9l65PichGEfmViJyd4XNE5D4RqRKRquTG3LW1tTQ0NIS60Tecvnn5sWPHAt3ou7WtjY7OTs9pqqmpCTxNhW5eXlZWVtJN5vOlqaysrKSbzGdLU319Pa+99hrLli2jrs5ZdHjz5s2h3XupaTp27FhJN5n3kqaGhobQ773UNNXW1oZ+72X6Xwjj3suWptT/hfQ05VMoE9xE5DacBffudd/fDcxLbTYSkTOAZlVtF5G/Av5UVa/LFW/qcNWjR48yatSokqXBiwJnSFskL3T/ImSawZT/kiVL6N+/PzNnzqSpqalX5oFNDKb9bWDI5W/LBLcaILUGUOmeOyVVPaaq7e7bp4HZHuMGnElDphUYQ/ow1bD9fcg0Q5j+7e3trF27llgsxhVXXMG8efPo27dvr8oDWxlM+9vA4Mc/V1NSRdrf2Q4vWg1MEZFJItIXuB14OTWAiJyV8vbjwBaPcQMYfzoIlKHAYaqB+/uQaYYw/FWVffv2sXjxhzvVpq5v1BvywHYG0/42MPjxD2WCm6rGgPuB3+L84L+oqtUi8oiIfNwN9rciUi0iG4C/xdkxzrMOHjxYSPCSyBdDpuW0PQ5TDcQ/IJlmCMP/gw8+oLq6mvnz5zNr1qzTFr3rDXlgO4NpfxsY/Ph7XXZ7GNChqq0iUgZ8Foip6k+Ldg5AqX0MsVjM+FLFBTOkbc3ZRUUslNct86Cb+Ksqu3fvprW1lWnTpqGqWYeg9tQ86E4Mpv1tYMjlH9Sy268CF7l/P4QzdPUxEXks6xUhq7q62jRC4QzphcKcORmX0y6ZfwlkmqEU/s3Nzbz55pvs3Lnz1FIWueYl9MQ86G4Mpv1tYPDj77XGcAwYrapxEdmF0wfQBCxX1QlFu/tUt152O7W24HHUUaRwlawVbNiwgb59+/LRj3401IlqkSKVSkHVGMrdQmEi0FdVq1X1AGB2BkmKTG+KUTBDslAooHM5UP8SyTRDUP4nTpzgjTfeoKmpienTp3PBBRd4LhR6Sh50ZwbT/jYwlHyjHhFZhtNxPAGnkPgLdxTRGlUdV7S7T3XLGkN6v0JUW7BKyUlA27dv5+KLL2by5MlRLSFSj1NQNYYv4Oz5PAV4xD13A/A7f3jByXTp7JmhyKGogfmXWKYZ/PgnEglisRjNzc0sXLiQc889t6hCoTvnQU9hMO1vA0O0tWd3UoGzmSOVXrFYjE2bNtHU1MTVV19tGidSpJIrqBoDInK2iHxNRP7NfS3dTiNFKLnOSG9mMO1vA0Oh/sk1jlpbW5k3b54RhqBl2t8GBtP+NjD48ffax3Al8DqwEdgFfASYDixS1WVFu/tUao2hvb2dfv36mULJzZBpvkIJagxW54Fl/p2dnfTp04fa2lpUlfHjM63pWFqGUsm0vw0Mpv1tYMjlH1SN4Z+Bv1XVy1X1blW9Aqff4X8XTFsi7d+/3zRCZoZMhULAfQs5/UOWaQYv/jU1NSxevJj6+nrGjRsXaKHglaGUMu1vA4NpfxsY/Ph7nZZ3AfBs2rnngX8p2jlgjRkzxjRCZobUYalFTFrz7R+yTDPk8o/H46xYsYLjx49z6aWXlozV5jzoLQym/W1g8OPvtcZQB8xKOzcLqC/aOWA1NjaaRjid4eGHP/y7xIVCRn8DMs2QyV9VaW5upqysjLFjx7Jo0aKS/tPamAe9jcG0vw0Mfvy91hieABaLyPeBPcA5wF8BD+e6KEyZ2CkrL0MJJrEV5G9AphnS/VtaWli9ejUdHR1cf/31TJ48OXSGsGXa3wYG0/42MPjx91QwqOp/iEgjzoqnn8LZje3/U9WfezUSkYU4BUw58LSqPp4l3KeAXwGXqGq3Gos68DvfgS0ZVgsPobYQ6XQdOnSIFStWcN555zF16tRoolqkSB6Vt2AQkXNxFtBbWUhBkBZHOfAUzqS4g8BqEXlZVTenhRsCfBFYWahHW1tbMWiBqnztWhgwoOvJkGoLYEcemGZoa2vjgw8+oE+fPgwfPpwFCxYwbNiw0BlMyrS/DQym/W1g8OOfs2AQkT8GfoHzlN8hIn+sqotzXZNFc4GdqrrbjfcF4BPA5rRw/wj8L+ArhRoMHz68CKxg1afC3bfI0OQ1G/LAJEMikeDo0aOsXr2aSy+9NPDRRl5l+nsw7W8Dg2l/Gxj8+OfrfP4H4BvAEOBb7t/FaDxO81NSB91zpyQis4CzVfXVXBGJyH0iUiUiVcmNuWtra9m+fXuoG33D6ZuXn2xupr29PZSNvjOlaePGjSXdZN5Lmurq6kq6yXy2NHV2dvKLX/yC3bt3M3bsWMaPH1+STea9pGnjxo2h33upadq2bVvo9156mrZv3x76vZeapvfffz+0ey9bmurq6kK/91LTVFdXlzVN+ZRzgpuINABnqGpCRCqAA6o6Nm+sp8dzG7BQVe91398NzFPV+933ZcCbwD2quldE3gb+Ll8fg1UT3B5+mPiqVZSXlRmrMRjPAwMM8XicgwcPMnHiRE6cOEG/fv2Md/qZ/h5M+9vAYNrfBoZSTnArV9UEgKp2An2LZKwBzk55X+meS2oIMA14W0T2ApcCL4uI5wb67du3F4nmU8ktOauqONncHGqfQrqM5YEhhqNHj/L666+zb98+4vE4w4YNY8eOHaH5Z5Pp78G0vw0Mpv1tYPDjn6/G0MaHq6mC07T0T6lhVDXvLm4i0gfYDizAKRBWA3eqasYthoqpMRhT6h7NIUxii+Sorq6Od999l9mzZ3P22WdHI44iRSpAfmsMK3BGEiWPlWnvr/cCoaox4H6cPR22AC+qarWIPCIiH/cSRz6FssRtsnaQeiT1yiusueWW0jPkkOllfsNgOHz4MIcPH+bMM8/k5ptvZsKECV0Khd6QB7b728Bg2t8GhmjZ7bCUWhB0BYlqCiVWR0cH69at4/Dhw8ybN4+xYwvu6ooUKZKrwJbdtl2hls6vvNL1cAuF7vyEYDvDe++9R1lZGTfffHPOQqEn50F38beBwbS/DQxRjSEsRZvshKq2tjbef/99ZsyYgYhQXl5uGilSpB6hXlNjSI4j7s0Mpv2DYlBV9uzZw+LFi+nTp09BhUJPyYPu7G8Dg2l/Gxj8+PeYGkMsFqNPH69rAhapPDWGUBhyyLR/UAwnTpzgvffeY+7cuYwcOTJ0f78yzWDa3wYG0/42MOTyD7TGII7OKpAvFO3cubO0Bg/nX0i25AyW+/thUFV27NjBpk2bGDZsGDfddFPBhYIf/yBlmsG0vw0Mpv1tYPDj76lgEJHBIvIjoBXY6Z77pIhYMxSnsrLEW1B7WEK75Ax5ZNq/WIampib+8Ic/sGfPHiZMmABQ9LyE7poHPcnfBgbT/jYw+PH3WmP4LjAGuALocM+tBv60aOeAdfTo0dJEnJy7kFSOYaklY/Ao0/6FMiSbMffs2cPZZ5/NDTfc4Hsl1O6WBz3R3wYG0/42MPjx99oAdgswVVVPiIgCqGqNiIwr2jlgDR48ONgIi9irOXCGAmXavxCGhoaGU6ugTp8+PXT/Uso0g2l/GxhM+9vA4Mffa8FQhtOMdEoiMhhoLto5YHV2dgYbYWqh4HECW+AMBcq0vxeGeDzO+++/z65du5gxYwZDhgwJ1T8MmWYw7W8Dg2l/Gxj8+HstGN4Bvk7XrTy/ALxVtHPASiQS/iPJVEsoYM5CIAw+ZNo/H0MikSCRSNDe3s6iRYsYkL6pUYn9w5JpBtP+NjCY9reBwY+/14LhAeBNEbkLGCwim3BWWr2uaOeANXDgQP+RFNh0VBIGHzLtn40huV59c3MzV199NXPnzg3VP2yZZjDtbwODaX8bGPz4e93z+YCITANuBc4B9gH/rar5d3wIScePH2fEiBHBRFbkzOZAGbqhfyaGuro6VqxYwZgxY7j00ktD9zch0wym/W1gMO1vA4Mff8+zL1S1HfhVUS6AiCwEnsDZJvRpVX087fO/Bj4PxHH6Lu5L3xM6l8aNM98PbprBtH8qQ0dHBxUVFcTjcebOnctZZ4Uz/cWmPOit/jYwmPa3gcGPv9d5DD/Idni8vhx4ClgETAXuEJGpacF+pqoXqeoM4J+BfykgHezZs6eQ4KfLwwS2kjN0c/8kw4EDB1i8eDH19fWMGzcutEIh6W9aphlM+9vAYNrfBgY//l7nMVSkHROBuwGvvYdzgZ2qultVO4AXgE+kBlDVppS3g4CC1ur46Ec/Wkjw0+VhAlvJGXzKtH88Hufo0aNs2LCBK664gjFjxoTOYDoPbGAw7W8Dg2l/Gxj8+HsqGFT1c2nHTcCdeB+uOh44kPL+oHuui0Tk8yKyC6fG8LeZIhKR+0SkSkSqkhtz19bW8u677xa10ffxL36RjoULaW1ro62tjaOf/3zRm5cvX748tI2+M6Xp7bffLukm89nSVFVVRVNTE+vWraOtrY3x48czYsSIkmwyny9N69evL+km817S9Pbbb5d0k/l8aVq+fHno9156mt59991Q7r1saVqyZEno9146z/r160O/91LTlPq/kJ6mfCp6ET1x1iw4pqp5F7QRkduAhap6r/v+bmCeqt6fJfydwE2q+me54vW97Hb68NRow52CdfLkSVatWkU8HmfBggXRFpuRInUDlXLZ7UWkTXrLoRrg7JT3le65bHoB+GQhMEVtSpHafJSy4U6x6s4bcxSjmpoaXn/9dUaPHs11112HiPS6PLCRwbS/DQym/W1gKPlGPSKyg65t/oOA0cAXVfXfPVzfB9gOLMApEFYDd6pqdUqYKaq6w/37VuBbuUo0CKDGEG28U5SampqoqKhAVYnFYgwdOtQ0UqRIkQpQUDWGfwIeTTm+AJzvpVAAUNUYcD/wW2AL8KKqVovIIyLycTfY/SJSLSLrcSbU5WxGSleyndCkTDOU2j+RSFBdXc3vf/97GhoaGDhw4GmFQk/Pg+7AYNrfBgbT/jYw+PHPW2Nwn/YfAJ5U1bainUqg1BpDIpGgrMxjOZfetxBQjaEghhKolP6qyhtvvEGfPn2YO3cugwYNCp3Bi0z728Bg2t8GBtP+NjDk8vddY3Cf9r9hW6GQrq1bt3oPnN7hbIKhBCqFfzweZ8+ePYgI8+bN45prrslaKJSKoRCZ9reBwbS/DQym/W1g8OPvdebzWyJytaouKdqpxJo0aVLhFwXct1AUg8X+R44cYeXKlQwfPpwJEyZ46kvoaXnQHRlM+9vAYNrfBgY//l7rOXuB34jI0yLyDyLyjeRRtHPAOnTokGkE4wxB+h8+fJjly5czffp0rrzySsrLy0NnKEam/W1gMO1vA4NpfxsY/PjnrDGISJOqDgVmAOuAye6RlAKPFe0eoIrZH7inMQThf+jQIcrKyhgzZgw333wzffv2DZ3Bj0z728Bg2t8GBtP+NjD48c9XYxAAVb02y2HNststLS2mEYwz+PFvb2/nvffeY/Xq1YgIIlJwoeCXIQiZ9reBwbS/DQym/W1g8OOfr4+huGnRBmR6BIINDH7833vvPYYMGcLHPvYx+vTxvOhuoAxByLS/DQym/W1gMO1vA4Mf/3y/AP1F5Me5AqjqnxftHqAqKipMIxhnKNS/tbWVTZs2MXPmTObPn++5HyFIhqBl2t8GBtP+NjCY9reBwY+/lyIlnuewQs3N5refNs3g1V9V2bVrF6+99hr9+/enrKwskEKhEIZSybS/DQym/W1gMO1vA4Mf/3w1hjZV/cuiYw9Ro0aNyh8o057OYTOUUF79m5qa2LlzJ9dee23gO0x1lzzoyQym/W1gMO1vA4Mff/MNcQHp4MGD+QOVaGJbQQwlVC5/VWXbtm1s3LiRYcOGceONN5Zk20Gb86C3MJj2t4HBtL8NDH78cy6JISIfqOqQomMvsVKXxIjFYvk7TUu8aJ4nhhIqm/+JEydYuXIlZWVlzJ07t6SL3tmaB72JwbS/DQym/W1gyOXva0kMmwuFdFVXV+cP1MMZ0v2Thf6+ffuYNGkSCxYsKPlKqLblQW9kMO1vA4NpfxsY/PgXvVFPwUYiC4EngHLgaVV9PO3zB4B7gRhwBPhzVd2XK86Clt1O7V/oBctsHzt2jNWrV3P55ZdHy2JHihSpi0q5UU8hEOXAUzib+0wF7hCRqWnB1gFzVPVi4Fc423t6Vt5NKQLY09k3Q4m1Zs0a4vE469atY8mSJZx//vkMGRJupc+GPDAt0wym/W1gMO1vA0PJN+rxKxG5DHjI3SsaEfk6gKp+O0v4mcC/qeoVueItqMbQCzblicfjJBIJ1q9fz0UXXUT//v1NI0WKFMlCWVFjAMYDB1LeH3TPZdNfAK9l+kBE7hORKhGpSm7MXVtby7Jly3Ju9N144gRQ2g3Zly5dGtpG36lxrFq1ilWrVvGTn/yEWCzGyJEjaW1tDXyTeS9pWrNmTUk3mc+XpiRDkGkq9Ht68803S7rJfL40LV26NLR7L1uali1bFvq9l5qmt956K/R7L9P/Qtj3XmqaUv8X0tOUT2HVGG4DFqrqve77u4F5qnp/hrB34ez2drWqtueKN6oxOKugrly5krFjxzJz5syi1jeKFClS75ItNYYa4OyU95XuuS4SkeuBvwc+nq9QSFeytDWpMBna29tPjTqaN28e8+bNY9u2baH5Z5Pp78G0vw0Mpv1tYDDtbwODH/+wagx9gO3AApwCYTVwp6pWp4SZidPpvFBVd3iJN7XG0N7eTr9+9EzaMwAAGuJJREFU/bIHDqHGkJchAKkq+/fvZ+3atVxxxRWMHj06VP98Ms1g2t8GBtP+NjCY9reBIZe/FTUGd3vQ+4HfAluAF1W1WkQeEZGPu8H+NzAY+KWIrBeRlwvx2L9/f6DMxajUDPF4nGXLlvH+++8zf/78LoVCGP5eZJrBtL8NDKb9bWAw7W8Dgx//0KblqepiYHHauW+m/H29n/jHjBnj5/JAVCoGVaWpqYlhw4YxceJEKisrMy5615PzoLv428Bg2t8GBtP+NjD48e8xayU1NjaefvLhh50mpGQzkgkGn/rggw948803qaqqQlWZOHFi1pVQS+FfqEwzmPa3gcG0vw0Mpv1tYPDjb3YxkQCVccx++oilEk5uy8rgQwcPHmTlypVMnTqV888/HxEJ1b8YmWYw7W8Dg2l/GxhM+9vA4Me/xxQMOdXNhqg2NjbSt29fzjjjDG688cbQZy9HihSpd6vHNCW1tbWZRvDNkEgk2LRpE2+++SaNjY0MGDCgoEKhJ+RBd/e3gcG0vw0Mpv1tYPDj32NqDMOHDzeN4ItBVXnjjTfo168fCxcuZODAgaH6ByXTDKb9bWAw7W8Dg2l/Gxj8+PeYGkNdXZ1phKIYYrEYu3fvRkS47LLLuOqqq4oqFIr1D1qmGUz728Bg2t8GBtP+NjD48e8xBcOECRNMIxTMUFdXx2uvvcbhw4eJx+MMGTIkbwdzkP6lkGkG0/42MJj2t4HBtL8NDH78e0zBsH37dtMIBTEcPnyYFStWMGvWLC6//PKsQ1BL5V8qmWYw7W8Dg2l/GxhM+9vA4Mc/tI16SqG8i+hZuHBeTU0NIsJZZ51FLBajoqLCNFKkSJF6maxYEiMMmd4UIx9DW1sby5cvZ+3atfTp0wcRCbxQsD0PeoO/DQym/W1gMO1vA4P1G/WUSllrDKnbeIIVNYYlS5YwdOhQLrroIuOblEeK5FVNTU3U19fT2dlpGiVSAaqoqGD06NFZt/XNV2PoMb9Qa9asYfbs2c6b1EKhxLOdszIALS0tbNq0iVmzZjF//nzKykpbQUv3NyHTDKb9bWAIyr+pqYm6ujrGjx/PgAEDChoYcfLkSQYNGuSboViZ9jfJoKqnNhiaOHFiUXu+h1ZjEJGFwBNAOfC0qj6e9vlVwPeAi4HbVfVX+eLMWmMw3LegquzatYuNGzdy3nnnMXXq1JIXCpEiBa2dO3cybty4oodPRzKrlpYWDh06xLnnnnvaZ1b0MYhIOfAUsAiYCtwhIlPTgu0H7gF+VoxHcms9k0oyNDU1sWfPHhYsWMC0adNCKxRsyoPe6m8DQ1D+nZ2dDBgwoKhrW1paAmEoVqb9bWBQ1aKbAMNqSpoL7FTV3QAi8gLwCWBzMoCq7nU/SxRjcOGFF57etxCiEokE5eXlbNiwgenTp3P99df7mpNQjC688MJQ/WxkMO1vA0OQ/sXew8UWKEHJtL8NDH5qemG1b4wHDqS8P+ieC0w7d+401rfQ2NjI73//ezZv3szkyZOB4v+h/Gjnzp2he9rGYNrfBgbT/tC91wnqKQx+/Ltdw7eI3CciVSJSVVtby9GjR6mtraWiooKOzk5OnjxJ64svsvlP/oREIsHatWuBD4durV27lkQiwebNm0910DQ0NFBTU0Myvr1799Lc3MzWrVuJxWKnqubJOJKvGzdupL29ndWrVzNu3DhmzJhBS0sL9fX17N+/n6amJnbs2EF7e/up/VfT49iwYQOxWIytW7fS3NzM3r17T6WppqaGhoYGdu3aRWtrK5s3b86Zpra2Nt9p2rRpE+3t7ezYsYOmpib2799PfX295zRVVlYGmqZCv6fKysrA01To99TW1lbyey9XmsrLywNJU/KeSiQStLa2oqqcPHkSoMtrsrMzGT4WiyEidHR00NnZSXt7O/F4/FQcySaW9LhaWlpOxRGPx2lvb6ezs7NLHEmeXHGoKqraJY6Ojg46OjqIxWJFpykZh9c09e3bN9A0tba2FpSmvn370tHRkfHey6tkJpbyAC4Dfpvy/uvA17OEfRa4zUu8s2fP1qT27NmjesstzhGCjhw5oq+++qqeOHGiK4NBmfa3gcG0vw0MQflv3ry56Gvb2toCYfCiq6++WocPH97Fc/78+frDH/6wS7i33npLx48ff+p9IpHQJ554Qi+88EIdOHCgjh8/Xm+77TbduHFjQf6JREK/+tWv6siRI3XkyJH61a9+VROJRMY8ePTRR3XQoEGnjv79+6uI6JEjR7qEO3bsmI4aNUqvuOKKU+d++tOfdrl2wIABCmhVVVVGrra2tqzfIVClOX5bw6oxrAamiMgkEekL3A4UtKdzPg0ePDjI6LIqFouxZs0ali1bxrRp07osix0WQzaZ9reBwbS/DQym/YHQBlzs3buXZcuWISK8/PKHPylemnK/+MUv8sQTT/Dkk09y/Phxtm/fzic/+UleffXVghh+8IMf8NJLL7FhwwY2btzIK6+8wve///2MefCNb3yD5ubmU8fXvvY1rrnmGkaNGtUl3Ne+9jUuuOCCLuc+85nPdLn23//93/nIRz7CrFmzMnL5+Q5C+fZUNQbcD/wW2AK8qKrVIvKIiHwcQEQuEZGDwJ8A3xeR6kI8wpiAE4/HAeemu/nmm5kwYUKXG9D0JCDT/jYwmPa3gcG0P5Cs/Zdczz//PJdeein33HMPzz33nOfrduzYwVNPPcXPf/5zrrvuOvr168fAgQP5zGc+w4MPPlgQw3PPPceXv/xlKisrGT9+PF/+8pd59tln8+aBqvL888/zZ3/2Z13Ov/vuu7z//vt87nOfy+v72c9+Nmsh6Oc7CG2Cm6ouBhannftmyt+rgcpi408kihrM5EkdHR2sW7eO9vZ2rrrqqqwldCkZvMi0vw0Mpv1tYCiZfwF7p/dJJKDYJ9YC5h89//zzPPDAA8ybN49LL72Uuro6xowZk/e6P/zhD1RWVjJ37tysYR5//HEef/zxrJ8n91Surq5m+vTpp85Pnz6d6ur8z7XLli2jvr6eT33qU6fOxeNx7r//fn74wx+e6hvKpH379rF06VJ+/OMf5/UpRt2u8zmbSjUJ59ChQyxevJiysjIuu+wyIwxeZdrfBgbT/jYwmPYHIIRRee+88w779u3j05/+NLNnz2by5Mn87GfepkEdO3aMs846K2eYBx98kMbGxqxHUs3NzQwbNuzU+2HDhtHc3Jy3Oeu5557jtttu69L09+STTzJv3ry8M9eff/555s+fz6RJk7KG8dOU1GOWxDh+/DgjAoyvra2Nfv36UVZWxuWXX87o0aO9MYwIkqIwmfa3gcG0vw0MJfMv4Em+o63N12b0XvTcc89x4403nmqfv/POO3nuuef40pe+RHl5+WlNap2dnacWrjzjjDOora0NhGPw4ME0NTWdet/U1MTgwYOJx+NZF8psaWnhl7/8Jb/5zW9OnTt06BBPPvmkp8Xvnn/+eb7xjW/kDBOLxTym4HT1mIJhwjPPBBKPqrJ3717WrVvHlVdeydixYz1fO27cuEAYipVpfxsYTPvbwGDaH6Bv374ljb+1tZUXX3yReDx+6n+0vb2dxsZGNmzYwMSJE9m7d2+Xa/bs2cPEiRMBWLBgAZ///OepqqpiTpY5T4899hiPPfZYVobm5mbAmVC4YcOGU81SGzZs4MILL8yZB7/+9a8ZOXIk11xzzalzq1atora2lqlTp55KY2trK2PHjqWmpubUni3Lly/n0KFD3HbbbTlyyN930GOaktreecf5w8fEtng8zpIlS9i6dSvXXHONp1pCqvbs2VO0dxAy7W8Dg2l/GxhM+4PzI11KvfTSS5SXl7N582bWr1/P+vXr2bJlC/Pnz+f555/nj/7oj3jmmWdYtWoVqsr27dv513/9V26//XYApkyZwt/8zd9wxx138Pbbb9PR0UFbWxsvvPDCqX6F9BFE6UdSn/3sZ/mXf/kXampqOHToEN/97ne55557cuZBpo7jRYsWsXfv3lPpeeSRR5g5cybr16/vspHXc889x6c+9akuIyIzydd3kGssq+3HqXkMDz2kiY99rOg5DIlEQhsaGlRVdd++fRqPx4uKp9jrgpJpfxsYTPvbwBCUv595DIlEIhCGbLrpppv0gQceOO38L37xCx0zZox2dHToj370I506daoOGTJEJ0+erN/+9re75E0ikdDvfe97OnXqVB0wYICOGzdOP/3pT+v7779fEEsikdCvfOUrOmLECB0xYoR+5Stf0UQicSoPBg0apEuXLj0V/uDBg1peXq47duzIGe8zzzzTZR6Dqmpra6sOGzZM33jjDU9cxc5j6Bn7Mdx6KydOnGDYggXwrW8VFEdTUxMrV66kvLyca6+91tdSFmvXrs06YikMmfa3gcG0vw0MQflv2bLltLH0XmV62WvT/jYwnDx5kv3792f8DvOtrtpjCgag4GW2Dxw4wKpVq5g2bRrnnXeekfWNIkWyVX4Khkh2KNt3aMWy22Go8cQJz2EbGhpoaWlh1KhRLFy4kPPPPz+QQqE7b+XXUxhM+9vAYNofPlznp7f628Dgx7/HFAzDU8YRZ1M8HmfDhg289dZbNDU1MWDAgECreqZ3DjPtbwODaX8bGEz7A8abcUz728Dgx7/HFAwn8tQYVJU33niDpqYmFi1aVNAwVK9KrqZpSqb9bWAw7W8DQ5D+xTY1d+en5Z7CkDpyqlD1mHkM2fY1jcVi7N27l8mTJ3PllVeWtBSfMWNGyeLuDv42MJj2t4EhKP+KigpaW1uLmklteva1aX8bGEQk6wS7fOoxNYZMpWNtbS2vvvoqR48eRVVLXrXbunVrSeO33d8GBtP+NjAE5T969GhqampO7QdQiLrzJjXdnUHd/SH27t1b8FyspHpMjSG9dK6trWXVqlXMnTs375ooQSnXuiW9wd8GBtP+NjD8/+2deZAU1R3HP9/ligerImC8EFQ8IKWlYmJVyjPRAo14YQSiAQu1PBONqTIVNSEYEzWVEFNiGW/R8oJKDCaSqBEisUTBKOgaDxbXyIIH6yoeoKK//PHeYM+4szuz9HTPWr9PVVd1T7/p7++96elfv/e6f7+09Au98JUrV1YdsdXMcn3KL2/9vG3o06cPgwcPLjuS0hWZOQZJo4GrgV7AjWZ2Rcn+fsBMYD+gDTjJYh7oSli3bh2bER5B7dWrF9tuuy1HHXUUvXtn5/tWrly5IbVnHuStXw825K1fDzakqd/Y2Niti0tzc3OubZC3fj3Y0NzczKBBg7r13UyGkiT1AmYAY4ARwARJI0qKTQHazWxXYDpwZTUa6zfZhAULFrBkyRL69u2LpEydAsCAAQMy1as3/XqwIW/9erAhb/16sCFv/XqwYWP0s5pj+DqwzMyWm9nHwN3AMSVljgEKmTZmA99SFf2wRdtsQ2NjI2PGjPlCNqSsKORszYu89evBhrz168GGvPXrwYa89evBho3Rz8oxbA+8ltheET/rsIyFjG/vAluXHkjSGZIWS1pcSKC+bt06Ri1fzpAhQ2hpaalJkvnkMcolZG9ra6tpkvmu6tTa2lrTJPOV1KmhoSHVOlX7OzU0NKRep2p/p9bW1szPvWSd2traMj/3SuvU3t6e+bmXrNOqVasyP/c6+i9kfe4l65T8L5TWqSsyCYkhaRww2sxOi9unAN8ws3MTZZ6LZVbE7eZYZnW5424IiQGsXr06t55CgbxtyFu/HmzIW78ebMhbvx5syFu/HmzoTL+rkBhZDcK3AjsmtneIn3VUZoWk3sAWhEnosjz11FOrJb0aNwcCZZ1IRuRtQ9769WBD3vr1YEPe+vVgQ9769WBDZ/o7dfbFrBzDImC4pGEEBzAemFhSZg4wCXgcGAc8Yl10Z8xsw5S7pMWdecAsyNuGvPXrwYa89evBhrz168GGvPXrwYaN0c/EMZjZeknnAv8gPK56s5k1SZpGiAs+B7gJuF3SMuBtgvNwHMdxMiaz5znN7AHggZLPfpZYXwecmJU9juM4Tsd8aUJiANfnbQD525C3PuRvQ976kL8NeetD/jbkrQ/529Bt/R6dqMdxHMdJny9Tj8FxHMdJAXcMjuM4ThE9zjFIGi3pRUnLJP2kg/39JN0T9z8haWgONhwk6T+S1seX+7LW/5Gk5yUtlfRPSZ0+s1wjG86U9KykZyT9u4PYWDXVT5Q7QZJJSv2xwQraYLKkt2IbPCPptCz1Y5nvxnOhSdKdaepXYoOk6Yn6vyTpnYz1h0iaJ+np+H84Mk39Cm3YKf4Pl0qaL2mHlPVvlvRmfEm4o/2S9Ido31JJ+3Z5UDPrMQvhUddmYGegL7AEGFFS5mzgurg+HrgnBxuGAnsRosWOy0H/UGDTuH5WTm3QmFgfC/w9S/1Yrj/wKLAQGJVDG0wGrklTt0r94cDTwFZxe3DWNpSUP4/wqHqWbXA9cFZcHwG05PA7zAImxfXDgNtTtuEgYF/guTL7jwTmAgIOAJ7o6pg9rcdQ82B8adhgZi1mthT4LEXdavTnmVkhgtZCwpvmWduwJrG5GZDmUw6VnAcAlxGi9NYiY0qlNtSKSvRPB2aYWTuAmb2Zgw1JJgB3ZaxvQCFu+BbAyhT1K7VhBPBIXJ/Xwf6NwsweJbz7VY5jgJkWWAhsKanTJDU9zTGkFoyvxjbUkmr1pxDuFjK3QdI5MebVVcAPstSP3eUdzexvKepWZUPkhNh9ny1pxw7211J/N2A3SY9JWqiQEyVNKj4X43DmMD6/QGalPxU4WdIKwntU56WoX6kNS4Dj4/pxQH9JaV6TuqLqa1ZPcwxOFUg6GRgF/CYPfTObYWa7ABcBl2SlK6kB+B1wYVaaZbgfGGpmewEP8XlPNit6E4aTDiHcrd8gacuMbSgwHphtZp9mrDsBuNXMdiAMqdwez48s+TFwsKSngYMJYYGyboeq6GmOoZpgfKjCYHw1sKGWVKQv6dvAxcBYM/soDxsS3A0cm6F+f+BrwHxJLYRx1TkpT0B32QZm1pZo+xsJ2Qkz0yfcGc4xs0/M7BXgJYKjyNKGAuNJdxipUv0pwL0AZvY48BVCcLnMbDCzlWZ2vJntQ/hPYmapTsJ3QfXXrDQnQWq9EO6AlhO6pIWJnpElZc6hePL53qxtSJS9lfQnnytpg30IE2LDc/wdhifWjybExMr8N4jl55P+5HMlbbBtYv04YGHG+qOB2+L6QMJwwtZZ/w7AHkAL8YXajNtgLjA5ru9JmGNIzY4KbRgINMT1y4FpabZDPO5Qyk8+H0Xx5POTXR4vbQNrvRC6gy/FC9/F8bNphDtjCHcEs4BlwJPAzjnYsD/hbu0DQm+lKWP9h4E3gGfiMieHNrgaaIr68zq6YNRSv6TsfFJ2DBW2wa9jGyyJbbBHxvoiDKk9DzwLjM+6DeL2VOCKtLUrbIMRwGPxN3gGOCIHG8YBL8cyNwL9Uta/C1gFfBKvO1OAM4EzE+fBjGjfs5X8FzwkhuM4jlNET5tjcBzHcWqMOwbHcRynCHcMjuM4ThHuGBzHcZwi3DE4juM4RbhjcOqeGJEyszen0yBGMz1pY8s4Th64Y3AyI17gP5L0fmK5MWebTNKH0ZbVkh6UtPfGHtfMRprZPVFjaNTZoVyZtElofhDr9qakP0saVsUxpkp6uBb2OfWNOwYnay4zs80TS6o5CrrJEWa2ObALIeji/Tnbkya7x7qNBLYEbsnZHqcH4I7ByR1J4yUtkbRG0ipJf5S0WZmy/SRdH++A10h6WdKJif0HKiQGeltSs6QLKw27bmbvEgLd7Shpa0mbSrpa0muxN3GfpCEldv9X0nuS3pB0W2JfSwxiCOGtW4AX4937paVlJC2SdH5JXadKmpfYPlbSU5Leibrfq6ResW5vEcLQb4gXJWlvSf+KdWuXNFfSLnHfScBPgUMSvbud475ut7HTM3DH4NQD7wITCXe0B8al3JzCJELIkT3NrJGQ+KQJQCFL3AOEaLKDCDFizgVOqcQISVsRkuu8YmZtwHRCbJkDgJ2A1cD9knpJ2hS4HTjHzPoTErWUGxYrDE3tHntJl3VQ5paoXbBFsa43x+3DgZuA84EBcd81kg6qsG5fBU4CXkx8bIRwFdsTYu28D9wBEIe4fgXMT/Tulm9sGzs9A3cMTtZcHO94C8sBZjbXzJrM7DMzWwZcC3yrzPc/BjYHRkjqbWavmdnzcd/ZwCwz+4uZfWpmLwDXAN/vwqa5CiknmwiB0I6OoZknAZeYWauZfUC4KO9JSM4CITbNHpIGmNkHZrage00ChHg3e0jaJ24fSnAAs+P2D4GrzWxBbKcnCRfxrurWJOk9QiydrYANvQwzW2ohqdNHsbf0C+CA6PTK0d02dnoQ7hicrLnczLZMLAslHS5pgUJ+5DWErGuDynz/DsKd+XSgTdKfJO0a9w0DJiQdD/BzoNNsVcCYaMt2ZjbWzJqifj/glUIhM3sfeJOQAOhDQvC00UBzHOKZ2K0WCcduB+4DTo0fnQrcbWZrE3W7qKRuk4Htujj0yNij2Z/gaDZMPkvaJbZfa2z3x+Kucm1fsKM7bez0INwxOLkiqS/hgng3MCQOD11EiAj5BcxsvZldaWajCMM7HxKHW4BXCTmFk46n0cxGdsO0t4CPCEMsBVs3BwbzeYbA+WY2lhBW+ZfAHYUx+hIqTfF6CzBR0kBCxq/kRPGrwNSSuvU3s4qS25vZYsLw3A2JHsF1wHvAXrHdv1moaid2p9nGTp3ijsHJm76EO/N2M1sbx7DPLVdY0mGS9pPUB1hLCG1eyIZ1LTBe0tGS+kjqLWmEpIOrNcrMPgNmApdJ2i5eTH8LvAA8KWkbSSdI2sJCVrJC4pWOMnO9RbjIdpUk56FYp5mEpPULE/t+D1wQJ357Seob26Ga5EMzCY60kGa1kdB+70RnNK2k/OvAkOi8C6TWxk794o7ByZU4PHMWcJWk9wlx4+/s5CvbECZ92wnj5jsBZ8RjPQd8hzAXsIow7HMrnQ+NdMYFwGJgEfA/wnDJ2OgIGghJoVriGP4MYJKZtXRQx7XApcBdcfjl4o7EEs5oDCWPlZrZg8DphEnf1bF+0wnzLRUR7Z5GGJLaKtbvQGANsAD4a8lXZhF6R69Hu4fVoI2dOsTzMTiO4zhFeI/BcRzHKcIdg+M4jlOEOwbHcRynCHcMjuM4ThHuGBzHcZwi3DE4juM4RbhjcBzHcYpwx+A4juMU8X9Vtl9cPHPSOQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDG3TrA3-TYX",
        "cellView": "form"
      },
      "source": [
        "#@title ARL.py {form-width:\"50px\"}\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "import torch.nn.init\n",
        "\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
        "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
        "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
        "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
        "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, alpha =0.001):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        attention = nn.Softmax2d(out)\n",
        "\n",
        "        out = out + residual + self.alpha * attention * residual\n",
        "\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, alpha = 0.001):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        attention = nn.Softmax2d()(out)\n",
        "\n",
        "        out = out + residual + self.alpha * attention *residual\n",
        "\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ARLNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1):\n",
        "        self.inplanes = 64\n",
        "        super(ARLNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])    #3\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2) #4\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)  #6\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)  #3\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc_ = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc_(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def arlnet18(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ARLNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
        "    if pretrained:\n",
        "        model_pretrained = model_zoo.load_url(model_urls['resnet18'])\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_dict = {k: v for k, v in model_pretrained.items() if k in model_dict}\n",
        "        model_dict.update(pretrained_dict)\n",
        "        model.load_state_dict(model_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def arlnet34(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ARLNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model_pretrained = model_zoo.load_url(model_urls['resnet34'])\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_dict = {k: v for k, v in model_pretrained.items() if k in model_dict}\n",
        "        model_dict.update(pretrained_dict)\n",
        "        model.load_state_dict(model_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def arlnet50(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-50 model.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ARLNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model_pretrained = model_zoo.load_url(model_urls['resnet50'])\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_dict = {k: v for k, v in model_pretrained.items() if k in model_dict}\n",
        "        model_dict.update(pretrained_dict)\n",
        "        model.load_state_dict(model_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def arlnet101(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ARLNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model_pretrained = model_zoo.load_url(model_urls['resnet101'])\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_dict = {k: v for k, v in model_pretrained.items() if k in model_dict}\n",
        "        model_dict.update(pretrained_dict)\n",
        "        model.load_state_dict(model_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def arlnet152(pretrained=False, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-152 model.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "    \"\"\"\n",
        "    model = ARLNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
        "    if pretrained:\n",
        "        model_pretrained = model_zoo.load_url(model_urls['resnet152'])\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_dict = {k: v for k, v in model_pretrained.items() if k in model_dict}\n",
        "        model_dict.update(pretrained_dict)\n",
        "        model.load_state_dict(model_dict)\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = arlnet50(pretrained=True)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QU5qM2H6-cdl",
        "cellView": "form"
      },
      "source": [
        "#@title resnet.py {form-width:\"50px\"}\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    __constants__ = ['downsample']\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        if groups != 1 or base_width != 64:\n",
        "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
        "        if dilation > 1:\n",
        "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    __constants__ = ['downsample']\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None):\n",
        "        super(ResNet, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        self._norm_layer = norm_layer\n",
        "\n",
        "        self.inplanes = 64\n",
        "        self.dilation = 1\n",
        "        if replace_stride_with_dilation is None:\n",
        "            # each element in the tuple indicates if we should replace\n",
        "            # the 2x2 stride with a dilated convolution instead\n",
        "            replace_stride_with_dilation = [False, False, False]\n",
        "        if len(replace_stride_with_dilation) != 3:\n",
        "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
        "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
        "        self.groups = groups\n",
        "        self.base_width = width_per_group\n",
        "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = norm_layer(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[0])\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[1])\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
        "                                       dilate=replace_stride_with_dilation[2])\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
        "        norm_layer = self._norm_layer\n",
        "        downsample = None\n",
        "        previous_dilation = self.dilation\n",
        "        if dilate:\n",
        "            self.dilation *= stride\n",
        "            stride = 1\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                norm_layer(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
        "                            self.base_width, previous_dilation, norm_layer))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
        "                                base_width=self.base_width, dilation=self.dilation,\n",
        "                                norm_layer=norm_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    # Allow for accessing forward method in a inherited class\n",
        "    forward = _forward\n",
        "\n",
        "\n",
        "def _resnet(arch, block, layers, progress, **kwargs):\n",
        "    model = ResNet(block, layers, **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet18(progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-18 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def resnet34(progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-34 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def resnet50(progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-50 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def resnet101(progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-101 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def resnet152(progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-152 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], progress,\n",
        "                   **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def resnext50_32x4d(progress=True, **kwargs):\n",
        "    r\"\"\"ResNeXt-50 32x4d model from\n",
        "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs['groups'] = 32\n",
        "    kwargs['width_per_group'] = 4\n",
        "    return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3], progress, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def resnext101_32x8d(progress=True, **kwargs):\n",
        "    r\"\"\"ResNeXt-101 32x8d model from\n",
        "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs['groups'] = 32\n",
        "    kwargs['width_per_group'] = 8\n",
        "    return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3], progress, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def wide_resnet50_2(progress=True, **kwargs):\n",
        "    r\"\"\"Wide ResNet-50-2 model from\n",
        "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n",
        "\n",
        "    The model is the same as ResNet except for the bottleneck number of channels\n",
        "    which is twice larger in every block. The number of channels in outer 1x1\n",
        "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
        "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs['width_per_group'] = 64 * 2\n",
        "    return _resnet('wide_resnet50_2', Bottleneck, [3, 4, 6, 3], progress, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def wide_resnet101_2(progress=True, **kwargs):\n",
        "    r\"\"\"Wide ResNet-101-2 model from\n",
        "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_\n",
        "\n",
        "    The model is the same as ResNet except for the bottleneck number of channels\n",
        "    which is twice larger in every block. The number of channels in outer 1x1\n",
        "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
        "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    kwargs['width_per_group'] = 64 * 2\n",
        "    return _resnet('wide_resnet101_2', Bottleneck, [3, 4, 23, 3], progress, **kwargs)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JliBbR5p_uaT",
        "cellView": "form",
        "outputId": "7e6fd5e4-4a6f-47ac-f4c3-c98c393b4a46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title training-validation split { form-width: \"50px\" }\n",
        "#374-melanoma\n",
        "#254-saborrheic\n",
        "#1372-nevus\n",
        "nev = 1372*0.3\n",
        "sab = 254*0.3\n",
        "mel = 374*0.3\n",
        "import shutil,os\n",
        "import pandas as pd\n",
        "files = os.listdir(base_path + 'After-Segmentation-2')\n",
        "\n",
        "gt = pd.read_csv(gt_path)\n",
        "\n",
        "for i in range(2000):\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzezRX6cvpHI"
      },
      "source": [
        "!unzip -d /content/gdrive/My\\ Drive/ISIC-2017-Org-Train-Data/ /content/gdrive/My\\ Drive/ISIC-2017-Org-Train-Data/ISIC-2017_Test_v2_Part1_GroundTruth.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_P7EfMBhMwE",
        "outputId": "645b9bd3-1000-43d7-d8dd-b7b8db7b8595",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from PIL import Image\n",
        "im = Image.open(base_path+'After-Enhancement-2/ISIC_0000000.jpg')\n",
        "im.size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(256, 256)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4A6QXzFEA5S",
        "outputId": "15b20fa9-c8f0-46c7-e3dd-235bdd8b0861",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls /content/gdrive/My\\ Drive/ISIC-2017-Org-Train-Data/ISIC-2017_Training_Data_Patch_2/ | wc -l"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "122000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQx7Naqkajx4"
      },
      "source": [
        "!unzip -d /content/gdrive/My\\ Drive/ISIC-2017-Org-Train-Data/ /content/gdrive/My\\ Drive/ISIC-2017-Org-Test-Data/ISIC-2017_Test_v2_Data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7JicIQcusJN"
      },
      "source": [
        "!mkdir /content/gdrive/My\\ Drive/ISIC-2017-Org-Train-Data/ISIC-2017_Training_Data_Patch/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3FtInShuuxd",
        "outputId": "a8bf9fbd-decb-4e66-c271-f087e6312ae6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!rm -rf /content/gdrive/My\\ Drive/ISIC-2017-Org-Train-Data/ISIC-2017_Training_Data_Patchasdasd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exXxwUUr5c2w",
        "outputId": "cc97da44-f3d8-4559-adce-d47c94bda353",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!du -sh /content/gdrive/My\\ Drive/ISIC-2017/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16G\t/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZJyVVnaMTlq",
        "outputId": "f3dc69fe-6c26-4709-c750-6d237b5fbb2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!du -sh /content/gdrive/My\\ Drive/train/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6.5G\t/content/gdrive/My Drive/train/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnjiP9quMZzM",
        "outputId": "f4866033-9c8e-4a24-d3c3-6221c7074753",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!du -sh /content/gdrive/My\\ Drive/test/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.6G\t/content/gdrive/My Drive/test/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b_HUxFAMpzj",
        "outputId": "2ddd2e34-5cb9-4703-b78b-6ea461efdfaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!du -sh /content/gdrive/My\\ Drive/validate/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.7G\t/content/gdrive/My Drive/validate/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0_9HfMCM_v_",
        "outputId": "4af22e76-009e-4b58-e72e-5b5567c71ca1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!du -sh /content/gdrive/My\\ Drive/ISIC-2017-Org-Train-Data/ISIC-2017_Training_Data_Patch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.7G\t/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/ISIC-2017_Training_Data_Patch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBnIc57CNf1D",
        "outputId": "f9050794-d392-4c91-9e0d-06b16df8e741",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from PIL import Image\n",
        "Image.open(base_path+'After-Enhancement-2/ISIC_0000000.jpg').size"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(256, 256)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29ofL12XtXYF",
        "outputId": "e5cc14ff-ae63-4470-f362-b50ca6a33ab3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "torch.cuda.current_device()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mwKVQ1MtYQ_",
        "outputId": "fae52bdf-3b16-447c-fd5a-5b44a9f42c3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ls /content/gdrive/My\\ Drive/ISIC-2017-Org-Train-Data/ISIC-2017_Training_Data | wc -l"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHBXIRwBtbEa",
        "cellView": "both",
        "outputId": "70992048-652d-4c1d-ae02-0cc643b25175",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Train_Mel-2.py  { form-width: \"50px\" }\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils import data\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "RANDOM_SEED = 6666\n",
        "\n",
        "\n",
        "def main():\n",
        "    np.random.seed(RANDOM_SEED)\n",
        "    torch.manual_seed(RANDOM_SEED)\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "    random.seed(RANDOM_SEED)\n",
        "\n",
        "    def train(model, dataloader, criterion, optimizer):\n",
        "        model.train()\n",
        "        losses = []\n",
        "        acc = 0.0\n",
        "        for index, (images, labels, _) in enumerate(dataloader):\n",
        "            labels = labels.to(device).unsqueeze(1).float()\n",
        "            images = images.to(device)\n",
        "            predictions = model(images)\n",
        "            loss = criterion(predictions, labels)\n",
        "            logps = F.logsigmoid(predictions)\n",
        "            ps_ = torch.exp(logps)\n",
        "            equals = torch.ge(ps_, 0.5).float() == labels\n",
        "            acc += equals.sum().item()\n",
        "            losses.append(loss.item())\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        train_loss = sum(losses) / len(losses)\n",
        "        train_acc = acc / len(dataloader.dataset)\n",
        "        print(f'\\ntrain_Accuracy: {train_acc:.5f}, train_Loss: {train_loss:.5f}')\n",
        "        return train_acc, train_loss\n",
        "\n",
        "    def validation(model, dataloader, criterion):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            running_acc = 0.0\n",
        "            val_losses = []\n",
        "            for index, (images, labels, _) in enumerate(dataloader, start=1):\n",
        "                labels = labels.to(device).unsqueeze(1).float()\n",
        "                images = images.to(device)\n",
        "                # hogs = hogs.to(device)\n",
        "                score = []\n",
        "                for i in range(len(images[0])):\n",
        "                    ps = model(images[:,i])\n",
        "                    score.append(ps)\n",
        "                score = sum(score) / len(score)\n",
        "                logps = F.logsigmoid(score)\n",
        "                ps_ = torch.exp(logps)\n",
        "                loss = criterion(score, labels)\n",
        "                val_losses.append(loss.item())\n",
        "                equals = torch.ge(ps_, 0.5).float() == labels\n",
        "                running_acc += equals.sum().item()\n",
        "            val_loss = sum(val_losses) / len(val_losses)\n",
        "            val_acc = running_acc / len(dataloader.dataset)\n",
        "            print(f'\\nval_Accuracy: {val_acc:.5f}, val_Loss: {val_loss:.5f}')\n",
        "        return val_acc, val_loss\n",
        "\n",
        "    def save_checkpoint(epoch,acc,lr):\n",
        "        filename = os.path.join(checkpoint_dir, \"mel_arlnet50_b32_best_acc.pkl\")\n",
        "        # torch.save(model.state_dict(), filename)\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'acc': acc,\n",
        "            'lr':lr\n",
        "            }, filename)\n",
        "\n",
        "    def adjust_learning_rate():\n",
        "        nonlocal lr\n",
        "        lr = lr / lr_decay\n",
        "        return optim.SGD(model.parameters(), lr, weight_decay=weight_decay, momentum=0.9)\n",
        "\n",
        "    # set the parameters\n",
        "    data_dir = '/content/gdrive/My Drive/ISIC-2017-Org-Train-Data'\n",
        "    # Create the dataloaders\n",
        "    batch_size = 32\n",
        "    # the checkpoint dir\n",
        "    checkpoint_dir = \"/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/checkpoint\"\n",
        "\n",
        "    # the learning rate para\n",
        "    lr = 1e-4\n",
        "    lr_decay = 2\n",
        "    weight_decay = 1e-4\n",
        "\n",
        "    stage = 0\n",
        "    start_epoch = 0\n",
        "    stage_epochs = [30, 30]\n",
        "    total_epochs = sum(stage_epochs)\n",
        "    writer_dir = os.path.join(checkpoint_dir, \"mel_arlnet50\")\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "\n",
        "    if not os.path.exists(writer_dir):\n",
        "        os.makedirs(writer_dir)\n",
        "\n",
        "    writer = SummaryWriter(writer_dir)\n",
        "\n",
        "    train_transforms = transforms.Compose([\n",
        "        # transforms.Resize((224, 224)),\n",
        "        transforms.RandomRotation((-10, 10)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.7079057, 0.59156483, 0.54687315],\n",
        "                             std=[0.09372108, 0.11136277, 0.12577087])\n",
        "    ])\n",
        "\n",
        "    val_transforms = argumentation_val()\n",
        "    # training dataset\n",
        "    train_dataset = ISICDataset(path=data_dir, mode=\"training\", crop=None, transform=train_transforms, task=\"mel\")\n",
        "    val_dataset = ISICDataset(path=data_dir, mode=\"validation\", crop=None, transform=val_transforms, task=\"mel\")\n",
        "    \n",
        "    # train_sampler = MySampler(train_dataset,last_index)\n",
        "    train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "    val_loader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
        "    # get the model\n",
        "    model = arlnet50(pretrained=True)\n",
        "\n",
        "    # the loss function\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    # the optimizer\n",
        "    optimizer = optim.SGD(model.parameters(), lr, weight_decay=weight_decay, momentum=0.9)\n",
        "\n",
        "    # initialize the accuracy\n",
        "    acc = 0.0\n",
        "    flag = True\n",
        "    if flag:\n",
        "          filename = os.path.join(checkpoint_dir, \"mel_arlnet50_b32_best_acc_third.pkl\")\n",
        "          checkpoint_t = torch.load(filename)\n",
        "          model.load_state_dict(checkpoint_t['model_state_dict'])\n",
        "          optimizer.load_state_dict(checkpoint_t['optimizer_state_dict'])\n",
        "          start_epoch = checkpoint_t['epoch']\n",
        "          lr = checkpoint_t['lr']\n",
        "          acc = checkpoint_t['acc']\n",
        "    for epoch in tqdm(range(start_epoch, total_epochs)):\n",
        "        train_acc, train_loss = train(model, train_loader, criterion, optimizer)\n",
        "        val_acc, val_loss = validation(model, val_loader, criterion)\n",
        "        writer.add_scalar(\"train acc\", train_acc, epoch)\n",
        "        writer.add_scalar(\"train loss\", train_loss, epoch)\n",
        "        writer.add_scalar(\"val accuracy\", val_acc, epoch)\n",
        "        writer.add_scalar(\"val loss\", val_loss, epoch)\n",
        "\n",
        "        if val_acc > acc or val_acc == acc:\n",
        "            acc = val_acc\n",
        "            print(\"save the checkpoint, the accuracy of validation is {}\".format(acc))\n",
        "            save_checkpoint(epoch,acc,lr)\n",
        "\n",
        "        if (epoch + 1) % 50 == 0:\n",
        "            torch.save(model.state_dict(), \"/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/checkpoint/mel_arlnet50/mel_arlnet50_b32_epoches_{}.pkl\".format(epoch + 1))\n",
        "\n",
        "        if (epoch + 1) in np.cumsum(stage_epochs)[:-1]:\n",
        "            stage += 1\n",
        "            optimizer = adjust_learning_rate()\n",
        "            print('Step into next stage')\n",
        "\n",
        "        if (epoch + 1) == 50:\n",
        "            torch.save(model.state_dict(), \"/content/gdrive/My Drive/ISIC-2017-Org-Train-Data/checkpoint/mel_arlnet50/mel_arlnet50_b32_epoches_{}.pkl\".format(epoch + 1))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/60 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train_Accuracy: 0.83075, train_Loss: 0.38668\n",
            "\n",
            "val_Accuracy: 0.80000, val_Loss: 0.51843\n",
            "save the checkpoint, the accuracy of validation is 0.8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  2%|▏         | 1/60 [21:24<21:03:30, 1284.92s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train_Accuracy: 0.88211, train_Loss: 0.27115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  3%|▎         | 2/60 [43:34<20:55:04, 1298.35s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "val_Accuracy: 0.78667, val_Loss: 0.48199\n",
            "\n",
            "train_Accuracy: 0.92994, train_Loss: 0.17194\n",
            "\n",
            "val_Accuracy: 0.80000, val_Loss: 0.58499\n",
            "save the checkpoint, the accuracy of validation is 0.8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  5%|▌         | 3/60 [1:05:05<20:31:19, 1296.14s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train_Accuracy: 0.95911, train_Loss: 0.10679\n",
            "\n",
            "val_Accuracy: 0.81333, val_Loss: 0.56831\n",
            "save the checkpoint, the accuracy of validation is 0.8133333333333334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  7%|▋         | 4/60 [1:27:13<20:18:31, 1305.55s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train_Accuracy: 0.97470, train_Loss: 0.06947\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  8%|▊         | 5/60 [1:49:18<20:02:19, 1311.62s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "val_Accuracy: 0.80000, val_Loss: 0.71583\n",
            "\n",
            "train_Accuracy: 0.98189, train_Loss: 0.04977\n",
            "\n",
            "val_Accuracy: 0.82000, val_Loss: 0.66260\n",
            "save the checkpoint, the accuracy of validation is 0.82\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 10%|█         | 6/60 [2:10:48<19:34:25, 1304.92s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train_Accuracy: 0.98693, train_Loss: 0.03694\n",
            "\n",
            "val_Accuracy: 0.82000, val_Loss: 0.74440\n",
            "save the checkpoint, the accuracy of validation is 0.82\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 12%|█▏        | 7/60 [2:32:56<19:18:52, 1311.93s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train_Accuracy: 0.98992, train_Loss: 0.02948\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 13%|█▎        | 8/60 [2:55:57<19:14:55, 1332.60s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "val_Accuracy: 0.80667, val_Loss: 0.76690\n",
            "\n",
            "train_Accuracy: 0.99210, train_Loss: 0.02344\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 15%|█▌        | 9/60 [3:17:25<18:41:26, 1319.34s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "val_Accuracy: 0.81333, val_Loss: 0.75345\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBc6F8bGBXuR",
        "cellView": "form"
      },
      "source": [
        "#@title MySampler { form-width:\"50px\" }\n",
        "import random\n",
        "from torch.utils.data.dataloader import Sampler\n",
        "\n",
        "random.seed(6666)\n",
        "\n",
        "class MySampler(Sampler):\n",
        "  def __init__(self,data,i=0):\n",
        "    random.shuffle(data)\n",
        "    self.seq = list(range(len(data)))[i*32]:\n",
        "  def __iter__(self):\n",
        "    return iter(self.seq)\n",
        "  def __len__(self):\n",
        "    return len(self.seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu_DvivOIrks"
      },
      "source": [
        "!cp /content/gdrive/My\\ Drive/ISIC-2017-Org-Train-Data/checkpoint/mel_arlnet50_b32_best_acc.pkl /content/gdrive/My\\ Drive/ISIC-2017-Org-Train-Data/checkpoint/mel_arlnet50_b32_best_acc_third.pkl"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLs9Y15N2woy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}